library(gdm)
pkg_root = '//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/gdmEngine'
## write DESCRIPTION file
DESCRIPTION = c('Package: gdmEngine',
'Version: 0.01',
paste('Date:', Sys.Date()),
'Title: Workflow for GDM',
'Description: Functions used to develop GDMs',
paste('Author:', unname(Sys.info()['user'])),
'Maintainer: Chris Ware <chris.ware@csiro.au>',
'SystemRequirements: git with shell distribution'
#paste('Authors@R:', unname(Sys.info()['user']))
)
sink(paste(pkg_root, 'DESCRIPTION', sep = '/'))
cat(DESCRIPTION, sep = '\n')
sink()
## Build with devtools
setwd(pkg_root)
document()
()
build()
install()
library(gdmEngine)
gitr.push(files = 'all')
library(assertthat)
gitr.push(files = 'all')
library(ALA4R)
library(raster)
library(gdmEngine)
library(data.table)
#library(dplyr)
library(magrittr)
library(plyr)
library(assertthat)
Aus.domain.mask <- raster("//ces-10-cdc/OSM_CDC_GISDATA_work/AUS0025/CLIM/MASK/MASK0.flt")
# SPECIFY ALA DATA FILTERING THRESHOLDS
data.start.year = 1970
location.uncertainty.limit = 2000
# Specify Environmental layers
climate.files <- list.files(path = "//lw-osm-02-cdc/OSM_CBR_LW_R51141_GPAA_work/ENV/A/OUT/1990", full.names=TRUE, pattern = ".flt")
terrain.files <- list.files(path = "//lw-osm-02-cdc/OSM_CBR_LW_R51141_GPAA_work/ENV/A/OUT/LAND", full.names=TRUE, pattern = ".flt")
env.files <- c(climate.files, terrain.files)
env.files <- env.files[(substr(env.files, nchar(env.files)-3, nchar(env.files)) == ".flt")] # to remove some arcmap filenames
env.files <- env.files[-c(20,21,32,35,36,38,39,40,43,44,45,46)] # remove grids we don't want to assess in the modelling
env.stk <- stack(env.files)
env.files
species.names.file <- "//osm-23-cdc/OSM_CBR_LW_DEE_work/source/biol/amphibians/AFD-20171211T130458.csv"
species.names <- read.csv(species.names.file)
species.names <- paste(species.names$GENUS, species.names$SPECIES)
species.names <- unique(species.names)
species.records.folder <- "//osm-23-cdc/OSM_CBR_LW_DEE_work/source/biol/amphibians"
species.records.folder.raw <- "//osm-23-cdc/OSM_CBR_LW_DEE_work/source/biol/amphibians/raw_files"
data.processing.folder <- "//osm-23-cdc/OSM_CBR_LW_DEE_work/processing/biol/amphibians"
agg.cell.rad <- 2.25
min.rich.limit <- 2
max.rich.limit <- 50
min.rich.rad <- 200
min.rich.proportion <- 0.25
n.pairs.model <- 100000
train.proportion <- 0.8
n.pairs.test <- 20000
Selected.records <- read.csv("//osm-23-cdc/OSM_CBR_LW_DEE_work/processing/biol/amphibians/selected_gridcell_composition_2017-12-14.csv")
Site.Env.Data <- read.csv("//osm-23-cdc/OSM_CBR_LW_DEE_work/processing/biol/amphibians/site_env_data_2018-01-05.csv")
Final.GDM <- gdm_builder(site.env.data = Site.Env.Data,
composition.data = Selected.records ,
geo=TRUE,
train.proportion = 0.8,
n.pairs.train = n.pairs.model,
n.pairs.test = n.pairs.test,
n.crossvalid.tests = 4,
correlation.threshold = 0.7,
selection.metric = 'RMSE',
Indiv.Dev.Explained.Min = 1.0,
n.predictors.min = 8,
output.folder = data.processing.folder,
output.name = "gdm_builder_FinMod")
decay.curve=function(x, a, b, c)
{
y = a + (1-a)/(1+((x/b)^(c)))
return(y)
} # end decay.curve
site.env.data <- Site.Env.Data
floor(nrow(site.env.data)/10)
n.pairs.target<-100000
n.pairs.sample<-floor(n.pairs.target/10)
n.pairs.total <- ((nrow(site.env.data)^2)-nrow(site.env.data))/2
floor(n.pairs.target/nrow(site.env.data))
b.used<-2*floor(n.pairs.target/nrow(site.env.data))
n.pairs.total
max(site.env.data$decimalLongitude)
min(site.env.data$decimalLongitude)
x.max<-max(site.env.data$decimalLongitude)
x.min<-min(site.env.data$decimalLongitude)
y.max<-max(site.env.data$decimalLatitude)
y.min<-min(site.env.data$decimalLatitude)
library(raster)
pairs.distance <- round(pointDistance(p1=c(x.max,y.max), p2=c(x.min,y.min), lonlat=T), 0)
max.distance.sites <- round(pointDistance(p1=c(x.max,y.max), p2=c(x.min,y.min), lonlat=T), 0)
b.dpair<-floor(max.distance.sites/3)
b.dpair<-floor(max.distance.sites/2)
n.pairs.target
n.pairs.sample
a.used=0.05
b.used
b.used<-2*floor(n.pairs.target/nrow(site.env.data))
b.used
c.used=3
a.dpair=0.05
b.dpair
c.dpair=3
train.pairs<-matrix(c(-3,-2,-1,0), nrow=2, ncol=2)
colnames(train.pairs)<-c("temp.i", "temp.j")
train.plot.weight.table <- data.frame("site.ID" = site.env.data$xy,
"ntimes.used" = 0,
"PairUse.Wt" = decay.curve(0, a.used, b.used, c.used))
i<-1
vals <- sample.int(nrow(site.env.data), (n.pairs.sample*2), replace=TRUE)
ij.pairs<-cbind(vals[c(1:n.pairs.sample)], vals[c((n.pairs.sample+1):(n.pairs.sample*2))])
temp.i<-rowMins(ij.pairs)
library(matrixStats)
temp.i<-rowMins(ij.pairs)
temp.j<-rowMaxs(ij.pairs)
ij.pairs<-cbind(temp.i,temp.j)
ij.pairs<-unique(ij.pairs)
ij.temp<-rbind(train.pairs, ij.pairs)
ij.temp<-unique(ij.temp)
ij.pairs<-ij.temp[c((nrow(train.pairs)+1):nrow(ij.temp)),]
n.pairs.selected<-nrow(ij.pairs)
pairs.distance <- round(pointDistance(p1=site.env.data[ij.pairs[,1],c(3:4)], p2=site.env.data[ij.pairs[,2],c(3:4)], lonlat=T), 0)
PairDist.Wt <- decay.curve(pairs.distance, a.dpair, b.dpair, c.dpair)
hist(PairDist.Wt)
Site.Pair.Prob <- PairDist.Wt * train.plot.weight.table$PairUse.Wt[ij.pairs[,1]] * train.plot.weight.table$PairUse.Wt[ij.pairs[,2]]
Pair.Sample <- rbinom(n=length(Site.Pair.Prob), size=1, prob=Site.Pair.Prob)
selected.ij.pairs <- ij.pairs[Pair.Sample>0,]
i.freq <- table(selected.ij.pairs)
i.freq <- as.data.frame(i.freq)
i.freq$selected.ij.pairs <- as.numeric(as.character(i.freq$selected.ij.pairs)) #
train.plot.weight.table$ntimes.used[i.freq$selected.ij.pairs] <- train.plot.weight.table$ntimes.used[i.freq$selected.ij.pairs] + i.freq$Freq
train.plot.weight.table$PairUse.Wt <- decay.curve(train.plot.weight.table$ntimes.used, a.used, b.used, c.used)
train.pairs<-rbind(train.pairs, selected.ij.pairs)
(nrow(train.pairs) >= (n.pairs.target + 2)
)
for(i in 1:1000)
{
# obtain row indices for randomly selected of pair-sites
vals <- sample.int(nrow(site.env.data), (n.pairs.sample*2), replace=TRUE)
# format the random sample of site indices into pairs
ij.pairs<-cbind(vals[c(1:n.pairs.sample)], vals[c((n.pairs.sample+1):(n.pairs.sample*2))])
# rearrange indices for each site pair so that the smalles comes first
temp.i<-rowMins(ij.pairs)
temp.j<-rowMaxs(ij.pairs)
ij.pairs<-cbind(temp.i,temp.j)
# omit duplicate site pairs within this sample (note: we wouldn't do this step under a bootstrapping approach)
ij.pairs<-unique(ij.pairs)
# and omit site pairs that have already been selected
ij.temp<-rbind(train.pairs, ij.pairs)
ij.temp<-unique(ij.temp)
ij.pairs<-ij.temp[c((nrow(train.pairs)+1):nrow(ij.temp)),]
# note how many unique sample pairs we've selected
n.pairs.selected<-nrow(ij.pairs)
# calculate the geographic distance (in metres) between sites in the pairs
pairs.distance <- round(pointDistance(p1=site.env.data[ij.pairs[,1],c(3:4)], p2=site.env.data[ij.pairs[,2],c(3:4)], lonlat=T), 0)
# determine the weight for each pair, based on the distance between sites
PairDist.Wt <- decay.curve(pairs.distance, a.dpair, b.dpair, c.dpair)
# And finally, calculate the total weight for each pair, combining the distance to NSW weights,
# the ntimes used weights, and the distance between sites in the pair weight
Site.Pair.Prob <- PairDist.Wt * train.plot.weight.table$PairUse.Wt[ij.pairs[,1]] * train.plot.weight.table$PairUse.Wt[ij.pairs[,2]]
# Use these probabilities to randomly select site-pairs
Pair.Sample <- rbinom(n=length(Site.Pair.Prob), size=1, prob=Site.Pair.Prob)
selected.ij.pairs <- ij.pairs[Pair.Sample>0,]
# update the weight's table ntimes selected for these sites
i.freq <- table(selected.ij.pairs)
i.freq <- as.data.frame(i.freq)
i.freq$selected.ij.pairs <- as.numeric(as.character(i.freq$selected.ij.pairs)) #
train.plot.weight.table$ntimes.used[i.freq$selected.ij.pairs] <- train.plot.weight.table$ntimes.used[i.freq$selected.ij.pairs] + i.freq$Freq
train.plot.weight.table$PairUse.Wt <- decay.curve(train.plot.weight.table$ntimes.used, a.used, b.used, c.used)
# and add these pairs to the main list of pairs selected for modelling
train.pairs<-rbind(train.pairs, selected.ij.pairs)
# check if we have enough pairs now (if so, randomly remove the necessary amount, so we hit our target,
# then break out of the loop)
if(nrow(train.pairs) >= (n.pairs.target + 2))
{
# remove the initial rows from train.pairs
train.pairs<-train.pairs[-c(1,2),]
# check how many excess pairs we have
n.excess <- nrow(train.pairs) - n.pairs.target
# Randomly select pairs to drop & remove them from the selected list
if(n.excess > 0)
{
drop.indices <- sample(seq_len(n.excess), size = n.excess, replace = FALSE)
train.pairs<-train.pairs[-drop.indices,]
}# end if n.excess > 0
# And break out of the loop
break()
}# end if nrow(train.pairs) >= (n.pairs.target + 2)
} # end for i
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
nrow(train.pairs)
Pairs.table <- data.frame(distance	= 0,
weights = 1,
s1.xCoord = site.env.data$decimalLongitude[train.pairs[,1]],
s1.yCoord = site.env.data$decimalLatitude[train.pairs[,1]],
s2.xCoord = site.env.data$decimalLongitude[train.pairs[,2]],
s2.yCoord = site.env.data$decimalLatitude[train.pairs[,2]])
sitepair_sample_geographic=function(site.env.data,
n.pairs.target,
n.pairs.sample=NULL,
a.used=0.05,
b.used=NULL,
c.used=3,
a.dpair=0.05,
b.dpair=NULL,
c.dpair=3,
output.folder = NULL,
output.name = "site_pairs_data_geo",
verbose=FALSE)
{
## Establish the working parameters ##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
n.pairs.total <- ((nrow(site.env.data)^2)-nrow(site.env.data))/2
if(is.null(n.pairs.sample))
{
n.pairs.sample<-floor(n.pairs.target/10)
}
if(is.null(b.used)) # If not specified, use twice the number of sites as the inflection point of the sampling function
{
b.used<-2*floor(n.pairs.target/nrow(site.env.data))
}
if(is.null(b.dpair)) # If not specified, use half the maximum distance across the domain
{
x.max<-max(site.env.data$decimalLongitude)
x.min<-min(site.env.data$decimalLongitude)
y.max<-max(site.env.data$decimalLatitude)
y.min<-min(site.env.data$decimalLatitude)
max.distance.sites <- round(pointDistance(p1=c(x.max,y.max), p2=c(x.min,y.min), lonlat=T), 0)
b.dpair<-floor(max.distance.sites/2)
}
# Create a table to catch the row indices for the pairs selected for modelling
train.pairs<-matrix(c(-3,-2,-1,0), nrow=2, ncol=2)
colnames(train.pairs)<-c("temp.i", "temp.j")
# Set up the site weighting table (site.ID, Dist2NSW.Wt, ntimes.used, PairUse.Wt)
train.plot.weight.table <- data.frame("site.ID" = site.env.data$xy,
"ntimes.used" = 0,
"PairUse.Wt" = decay.curve(0, a.used, b.used, c.used))
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
## Implement the sampling code, looping over sets of candidate pairs, sampling those,  and continuing until the
## specified number of pairs has been sampled (or it just takes too long - >1000 candidate samples). ~~~~~~~~##
# now loop until we have enough pairs randomly sampled
for(i in 1:1000)
{
# obtain row indices for randomly selected of pair-sites
vals <- sample.int(nrow(site.env.data), (n.pairs.sample*2), replace=TRUE)
# format the random sample of site indices into pairs
ij.pairs<-cbind(vals[c(1:n.pairs.sample)], vals[c((n.pairs.sample+1):(n.pairs.sample*2))])
# rearrange indices for each site pair so that the smalles comes first
temp.i<-rowMins(ij.pairs)
temp.j<-rowMaxs(ij.pairs)
ij.pairs<-cbind(temp.i,temp.j)
# omit duplicate site pairs within this sample (note: we wouldn't do this step under a bootstrapping approach)
ij.pairs<-unique(ij.pairs)
# and omit site pairs that have already been selected
ij.temp<-rbind(train.pairs, ij.pairs)
ij.temp<-unique(ij.temp)
ij.pairs<-ij.temp[c((nrow(train.pairs)+1):nrow(ij.temp)),]
# note how many unique sample pairs we've selected
n.pairs.selected<-nrow(ij.pairs)
# calculate the geographic distance (in metres) between sites in the pairs
pairs.distance <- round(pointDistance(p1=site.env.data[ij.pairs[,1],c(3:4)], p2=site.env.data[ij.pairs[,2],c(3:4)], lonlat=T), 0)
# determine the weight for each pair, based on the distance between sites
PairDist.Wt <- decay.curve(pairs.distance, a.dpair, b.dpair, c.dpair)
# And finally, calculate the total weight for each pair, combining the distance to NSW weights,
# the ntimes used weights, and the distance between sites in the pair weight
Site.Pair.Prob <- PairDist.Wt * train.plot.weight.table$PairUse.Wt[ij.pairs[,1]] * train.plot.weight.table$PairUse.Wt[ij.pairs[,2]]
# Use these probabilities to randomly select site-pairs
Pair.Sample <- rbinom(n=length(Site.Pair.Prob), size=1, prob=Site.Pair.Prob)
selected.ij.pairs <- ij.pairs[Pair.Sample>0,]
# update the weight's table ntimes selected for these sites
i.freq <- table(selected.ij.pairs)
i.freq <- as.data.frame(i.freq)
i.freq$selected.ij.pairs <- as.numeric(as.character(i.freq$selected.ij.pairs)) #
train.plot.weight.table$ntimes.used[i.freq$selected.ij.pairs] <- train.plot.weight.table$ntimes.used[i.freq$selected.ij.pairs] + i.freq$Freq
train.plot.weight.table$PairUse.Wt <- decay.curve(train.plot.weight.table$ntimes.used, a.used, b.used, c.used)
# and add these pairs to the main list of pairs selected for modelling
train.pairs<-rbind(train.pairs, selected.ij.pairs)
# check if we have enough pairs now (if so, randomly remove the necessary amount, so we hit our target,
# then break out of the loop)
if(nrow(train.pairs) >= (n.pairs.target + 2))
{
# remove the initial rows from train.pairs
train.pairs<-train.pairs[-c(1,2),]
# check how many excess pairs we have
n.excess <- nrow(train.pairs) - n.pairs.target
# Randomly select pairs to drop & remove them from the selected list
if(n.excess > 0)
{
drop.indices <- sample(seq_len(n.excess), size = n.excess, replace = FALSE)
train.pairs<-train.pairs[-drop.indices,]
}# end if n.excess > 0
# And break out of the loop
break()
}# end if nrow(train.pairs) >= (n.pairs.target + 2)
} # end for i
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
## Processing the selected pairs ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
# Provide a warning if not enough pairs were selected
if(nrow(train.pairs) < n.pairs.target)
{
# remove the initial rows from train.pairs
train.pairs<-train.pairs[-c(1,2),]
warning("less pairs selected than desired", call. = FALSE)
}
# Prepare the start of a GDM input table for the pairs selected
Pairs.table <- data.frame(distance	= 0,
weights = 1,
s1.xCoord = site.env.data$decimalLongitude[train.pairs[,1]],
s1.yCoord = site.env.data$decimalLatitude[train.pairs[,1]],
s2.xCoord = site.env.data$decimalLongitude[train.pairs[,2]],
s2.yCoord = site.env.data$decimalLatitude[train.pairs[,2]])
# return the selected pairs
return(Pairs.table)
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
}# end sitepair_sample_geographic
##-------------------------------------------------------------------------------------------------------------##
n.pairs.model
sitepair_sample_geographic(site.env.data = Site.Env.Data,
n.pairs.target = n.pairs.model,
a.used=0.05,
b.used=10,
c.used=3,
a.dpair=0.05,
b.dpair=1000000,
c.dpair=3)
Pairs.Table.Geo <- sitepair_sample_geographic(site.env.data = Site.Env.Data,
n.pairs.target = n.pairs.model,
a.used=0.05,
b.used=10,
c.used=3,
a.dpair=0.05,
b.dpair=1000000,
c.dpair=3)
sink(paste(pkg_root, 'DESCRIPTION', sep = '/'))
cat(DESCRIPTION, sep = '\n')
sink()
setwd(pkg_root)
document()
gitr.push(files = 'all')
gitr.push(files = 'all')
library(gdmEngine)
gitr.push(files = 'all')
gitr.push = function(verbose = TRUE, ...){
if("package:gdmEngine" %in% search()){
## exec files should be on path of package
sh_path = paste(.libPaths(), 'gdmEngine/exec/gitr.sh', sep = '/')
bat_path = paste(.libPaths(), 'gdmEngine/exec/gitr.bat', sep = '/')
assert_that(file.exists(sh_path))
assert_that(file.exists(bat_path))
} else {
## find these paths
sh_path = "//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/lib/gdmEngine/exec/gitr.sh"
bat_path = "//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/lib/gdmEngine/exec/gitr.bat"
assert_that(file.exists(sh_path))
assert_that(file.exists(bat_path))
}
## account for any whitespace
bat_path = paste('"', bat_path, '"', sep = '')
sh_path = paste('"', sh_path, '"', sep = '')
user_opts <- list(...)
## check for user config
opts = gitr.config()
## check for sh.exe
if(!file.exists(opts[['shell_exe']])) stop('Cannot find git - install it!')
## update opts with user options
if (length(user_opts)){
opts = gitr.config()
for (i in 1:length(user_opts)) {
this_opt <- names(user_opts)[i]
if (! (this_opt %in% names(opts))) {
cat(paste0("'", this_opt, "'", ' is not a valid option. Should be one of:'),
names(opts), sep = '\n')
} else {
opts[[this_opt]] = paste(user_opts[i])
}
} # end user_opts
}
## check files to be tracked
assert_that(length(opts$files) == 1)
if(opts$files == 'all') {
opts$files = '--all'
} else if(opts$files != 'GDM_Workflow_Functions.R') {
test_path = paste(opts$repo_root, opts$files, sep = '/')
if(!file.exists(test_path))
stop(paste("'", opts$files, "'", ' is not a path relative to ', opts$repo_root))
}
## msg arg must not be empty
assert_that(length(opts$msg) == 1)
## current workaround for messages and white space which doesn't seem to
## flow from batch to bash...
opts$msg = gsub("[[:space:]]", '_', paste(opts$msg))
## args
send_args = paste(
bat_path,
sh_path,
opts$repo_root,
opts$files,
opts$msg,
sep = ' ')
## run
if (verbose) {
system(send_args, intern = TRUE)
} else {
system(send_args, intern = FALSE)
}
}
gitr.push(files = 'all')
xx<-c('a','b')
is.vector(xx)
env.colnames<-c('PTA','TXX')
a.epair=0.05
b.epair=NULL
c.epair=3
n.pairs.total <- ((nrow(site.env.data)^2)-nrow(site.env.data))/2
if(is.null(n.pairs.sample))
{
n.pairs.sample<-floor(n.pairs.target/10)
}
if(is.null(b.used)) # If not specified, use twice the number of sites as the inflection point of the sampling function
{
b.used<-2*floor(n.pairs.target/nrow(site.env.data))
}
if(is.null(b.epair)) # If not specified, use half the maximum env distance
{
b.epair<-0.5
}
# Create a table to catch the row indices for the pairs selected for modelling
train.pairs<-matrix(c(-3,-2,-1,0), nrow=2, ncol=2)
colnames(train.pairs)<-c("temp.i", "temp.j")
# Set up the site weighting table (site.ID, Dist2NSW.Wt, ntimes.used, PairUse.Wt)
train.plot.weight.table <- data.frame("site.ID" = site.env.data$xy,
"ntimes.used" = 0,
"PairUse.Wt" = decay.curve(0, a.used, b.used, c.used))
colnames(site.env.data)
which(colnames(site.env.data) == env.colnames)
env.colnames<-c('PTA','TXXX')
which(colnames(site.env.data) == env.colnames)
colnames(site.env.data)[13]
env.colnames<-c('PTAX','TXXX')
which(colnames(site.env.data) == env.colnames)
zz<-which(colnames(site.env.data) == env.colnames)
zz
is.null(zz)
length(zz)
env.colnames<-c('PTA','TXX')
oldw <- getOption("warn")
options(warn = -1)
env.cols <- which(colnames(site.env.data) == env.colnames)
options(warn = oldw)
env.cols
length(env.cols)<1
env.colnames<-c('PTAX','TXXX')
oldw <- getOption("warn")
options(warn = -1)
env.cols <- which(colnames(site.env.data) == env.colnames)
options(warn = oldw)
length(env.cols)<1
env.colnames<-c('PTA','TXX')
oldw <- getOption("warn")
options(warn = -1)
env.cols <- which(colnames(site.env.data) == env.colnames)
options(warn = oldw)
env.cols
std.site.env.vars <- data.frame(xy = site.env.data$xy, site.env.data[,env.cols])
site.env.data[c(1:5),c(1:3)]
std.site.env.vars[c(1:5),c(1:3)]
zz<-scale(std.site.env.vars[,2])
hist(zz)
summary(zz)
variance(zz)
sd(zz)
plot(zz~std.site.env.vars[,2])
hist(std.site.env.vars[,2])
zz<-scale(std.site.env.vars[,env.cols])
zz<-scale(site.env.data[,env.cols])
zz<-scale(std.site.env.vars[,c(1:ncol(std.site.env.vars))])
zz<-scale(std.site.env.vars[,c(2:ncol(std.site.env.vars))])
plot(zz[,1]~std.site.env.vars[,2])
plot(zz[,2]~std.site.env.vars[,3])
std.site.env.vars <- data.frame(xy = site.env.data$xy, site.env.data[,env.cols])
std.site.env.vars[,c(2:ncol(std.site.env.vars))]<-scale(std.site.env.vars[,c(2:ncol(std.site.env.vars))])
env.colnames<-'PTA'
oldw <- getOption("warn")
options(warn = -1)
env.cols <- which(colnames(site.env.data) == env.colnames)
options(warn = oldw)
length(env.cols)<1
std.site.env.vars <- data.frame(xy = site.env.data$xy, site.env.data[,env.cols])
std.site.env.vars[,c(2:ncol(std.site.env.vars))]<-scale(std.site.env.vars[,c(2:ncol(std.site.env.vars))])
env.cols
head(std.site.env.vars)
plot(std.site.env.vars[,2]~site.env.data[,env.cols])
env.colnames<-c('PTA','TXX')
oldw <- getOption("warn")
options(warn = -1)
env.cols <- which(colnames(site.env.data) == env.colnames)
options(warn = oldw)
std.site.env.vars <- data.frame(xy = site.env.data$xy, site.env.data[,env.cols])
std.site.env.vars[,c(2:ncol(std.site.env.vars))]<-scale(std.site.env.vars[,c(2:ncol(std.site.env.vars))])
dist.method="euclidean"
zz<-dist(std.site.env.vars[,-1],method=dist.method)
install.packages("Rcpp")
library(Rcpp)
install.packages("Rcpp")
library(raster)
library(Rcpp)
