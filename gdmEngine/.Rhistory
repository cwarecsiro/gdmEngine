cat(paste(msg1, msg2, msg3, sep = '\n'))
}
}# end if verbose
# Return the freshly filled site-pair table
return(pairs.table.new)
} # end calculate_dissimilarities
##-------------------------------------------------------------------------------------------------------------##
# #'@title Compositional dissimilarity calculation for specified pairs of sites
# #'
# #'@description Calculate the dissimilarity between specified pairs of sites.
# #'
# #'@param site_spp (matrix) A matrix of the indices for species (col2) occurring in each site (col1), with each row an occurrence
# #'@param pair_rows (matrix) A matrix of the site index for site 1 (col 1) and site 2 (col 2) in each pair of sites (rows)
# #'@param site_rich (vector) A vector giving the total number of species in each site (element)
# #'@param max_richness (integer)
# #'
# #'@returns Vector, the Sorensen dissimilarity between the specified pairs.
# #'
# #'@examples output = PairsDissim(site.spp, ij.pairs, site.rich, max.rich)
# #'@importFrom Rcpp cppFunction
# #'
# #'@export
# cppFunction('NumericVector PairsDissim(IntegerMatrix site_spp, IntegerMatrix pair_rows, IntegerVector site_rich, int max_richness) {
#
#   int n_sites = site_rich.size();
#   int n_pairs = pair_rows.nrow();
#   int n_records = site_spp.nrow();
#   IntegerMatrix comp(n_sites,max_richness);
#   IntegerVector upto_index(n_sites);
#   NumericVector out(n_pairs);
#
#
#   for(int i_site=0; i_site < n_sites; i_site++) {
#     upto_index[i_site] = 0;
#     }
#   for(int i_rec = 0; i_rec < n_records; i_rec++) {
#     int site_index = site_spp(i_rec,0) - 1;
#     comp(site_index, upto_index[site_index]) = site_spp(i_rec,1);
#     upto_index[site_index] += 1;
#     }
#
#   for(int i = 0; i < n_pairs; i++) {
#     int site_one_index = pair_rows(i,0) - 1;
#     int site_two_index = pair_rows(i,1) - 1;
#     float n_spp_common = 0;
#     for(int i_spp_one=0; i_spp_one<site_rich[site_one_index]; i_spp_one++){
#       for(int i_spp_two=0; i_spp_two<site_rich[site_two_index]; i_spp_two++){
#         if(comp(site_one_index,i_spp_one) == comp(site_two_index,i_spp_two)){
#           n_spp_common += 1;
#           }
#         }
#       }
#     float sum_rich = site_rich[site_one_index] + site_rich[site_two_index];
#     out[i] = (1 - ((2 * n_spp_common) / (sum_rich)));
#     }
#   return out;
#   }')
# ##-------------------------------------------------------------------------------------------------------------##
#'@title Calculate dissimilarities
#'
#'@description Calculates the compositional dissimilarity between all specified site-pairs. Fills the dissimilarities in the input dataframe 'pairs.table'. Note this uses an approach that trades processing time for memory efficiency, so it can handle very large numbers of site pairs on any machine.
#'
#'@param pairs.table (dataframe) A dataframe holding the site-pairs. This is the first six columns of a gdm input table, plus 4 columns hold s1 & s2 long & lat.
#'@param composition.data (dataframe) A dataframe holding the final species composition data: the records of all species in all grid cells to be used for modelling.
#'@param output.folder (string) A folder to save the outputs to. If none specified, no file is written.
#'@param output.name (string) A name to use in saving the outputs. Default: 'pairs_table_dissim'.
#'@param verbose (boolean) Print messages to console. Default TRUE.
#'
#'@return Dataframe.
#'
#'@examples output = calculate_dissimilarities(My.pairs.table, My.composition.data, output.folder = 'C:/Users/processed_data', output.name = 'My.sitepair.data')
#'
#'@importFrom Rcpp evalCpp
#'@useDynLib gdmEngine
#'@export
calculate_dissimilarities <- function(pairs.table,
composition.data,
output.folder = NULL,
output.name = "pairs_table_dissim",
verbose=TRUE)
{
# First indexify the sites and species in composition.data
composition.data$Site.ID <- as.factor(paste(composition.data$decimalLongitude, composition.data$decimalLatitude, sep = '_'))
composition.data.site.indices<-levels(composition.data$Site.ID)
composition.data.spp.indices<-levels(composition.data$scientificName)
composition.data$site.index <- match(composition.data$Site.ID, composition.data.site.indices)
composition.data$spp.index <- match(composition.data$scientificName, composition.data.spp.indices)
# generalise...
#idx = sort(unlist(lapply(c('site.indices', 'spp.indices'), function(x)
idx = sort(unlist(lapply(c('site.index', 'spp.index'), function(x)
grep(x, names(composition.data)))))
#site.spp.index <- as.matrix(composition.data[,c(5,6)])
site.spp.index <- as.matrix(composition.data[,c(idx)])
# Then get the index for each site in the pairs.table
pairs.table$s1.site.ID <- paste(pairs.table$s1.decimalLongitude, pairs.table$s1.decimalLatitude, sep = '_')
pairs.table$s2.site.ID <- paste(pairs.table$s2.decimalLongitude, pairs.table$s2.decimalLatitude, sep = '_')
pairs.table$S1.index <- match(pairs.table$s1.site.ID, composition.data.site.indices)
pairs.table$S2.index <- match(pairs.table$s2.site.ID, composition.data.site.indices)
# generalise...
idx = sort(unlist(lapply(c('S1.index', 'S2.index'), function(x)
grep(x, names(pairs.table)))))
#pairs.site.index <- as.matrix(pairs.table[,c(13,14)])
pairs.site.index <- as.matrix(pairs.table[,c(idx)])
# Determine the richness of each site
site.richness <- tabulate(composition.data$site.index)
max.richness <- as.integer(max(site.richness))
# Now run some rcpp code to format the data & calculate dissimilarities for the selected pairs
distance <- PairsDissim(site.spp.index,
pairs.site.index,
site.richness,
max.richness)
# create a new dataframe to return, with the scaled dissimilarities
# don't know what they'll be... so just keep them
#pairs.table.new <- pairs.table[,-c(11:14)] # If we want to remove the xy site names
pairs.table.new <- pairs.table
pairs.table.new$distance <- distance
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -#
# Now write out the data to file (if specified) and return the aggregated records
# write the data to file, if an output folder is specified
if(!is.null(output.folder))
{
if(!dir.exists(output.folder))
{
dir.create(output.folder)
}# end if !dir.exists
out.path <- file.path(output.folder,paste0(output.name,"_",Sys.Date(),".csv"))
write.csv(pairs.table.new, out.path, row.names=FALSE)
# write a log file describing how the data was created *************************************
fileConn<-file(file.path(output.folder,paste0(output.name,"_",Sys.Date(),"_log_file.txt")),'w')
writeLines("#######################################################################",con = fileConn)
writeLines("###",con = fileConn)
writeLines("### Calculate dissimilarities log file ",con = fileConn)
writeLines("###",con = fileConn)
writeLines(paste0("### Created ",Sys.time()," using the calculate_dissimilarities() function."),con = fileConn)
writeLines("###",con = fileConn)
writeLines("#######################################################################",con = fileConn)
writeLines("",con = fileConn)
writeLines(paste0("Output data file = ", out.path),con = fileConn)
writeLines(paste0("Number of site-pairs = ", nrow(pairs.table.new)),con = fileConn)
writeLines(paste0("Dissimilarity metric = Sorensen's"),con = fileConn)
writeLines(paste0("Average site-pair dissimilarity = ", mean(pairs.table.new$distance)),con = fileConn)
writeLines(paste0("Proportion of site-pairs with non-complete dissimilarity = ", (sum(pairs.table.new$distance < 1)/nrow(pairs.table.new))),con = fileConn)
writeLines("#######################################################################",con = fileConn)
close(fileConn) #**************************************************************************
} # end if !is.null(output.folder)
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -#
# write some feedback to the terminal
if(verbose)
{
msg1 = 'Returned object is a dataframe.'
msg2 = paste('Dissimilarities have been calculated for ', nrow(pairs.table.new), ' site-pairs.')
msg3 = paste(((sum(pairs.table.new$distance < 1)/nrow(pairs.table.new))*100) , '% of site-pairs have non-complete dissimilarity.')
if(!is.null(output.folder)){
msg4 = paste('These data have been also been written to ', out.path)
cat(paste(msg1, msg2, msg3, msg4, sep = '\n'))
}else{
cat(paste(msg1, msg2, msg3, sep = '\n'))
}
}# end if verbose
# Return the freshly filled site-pair table
return(pairs.table.new)
} # end calculate_dissimilarities
##-------------------------------------------------------------------------------------------------------------##
# #'@title Compositional dissimilarity calculation for specified pairs of sites
# #'
# #'@description Calculate the dissimilarity between specified pairs of sites.
# #'
# #'@param site_spp (matrix) A matrix of the indices for species (col2) occurring in each site (col1), with each row an occurrence
# #'@param pair_rows (matrix) A matrix of the site index for site 1 (col 1) and site 2 (col 2) in each pair of sites (rows)
# #'@param site_rich (vector) A vector giving the total number of species in each site (element)
# #'@param max_richness (integer)
# #'
# #'@returns Vector, the Sorensen dissimilarity between the specified pairs.
# #'
# #'@examples output = PairsDissim(site.spp, ij.pairs, site.rich, max.rich)
# #'@importFrom Rcpp cppFunction
# #'
# #'@export
# cppFunction('NumericVector PairsDissim(IntegerMatrix site_spp, IntegerMatrix pair_rows, IntegerVector site_rich, int max_richness) {
#
#   int n_sites = site_rich.size();
#   int n_pairs = pair_rows.nrow();
#   int n_records = site_spp.nrow();
#   IntegerMatrix comp(n_sites,max_richness);
#   IntegerVector upto_index(n_sites);
#   NumericVector out(n_pairs);
#
#
#   for(int i_site=0; i_site < n_sites; i_site++) {
#     upto_index[i_site] = 0;
#     }
#   for(int i_rec = 0; i_rec < n_records; i_rec++) {
#     int site_index = site_spp(i_rec,0) - 1;
#     comp(site_index, upto_index[site_index]) = site_spp(i_rec,1);
#     upto_index[site_index] += 1;
#     }
#
#   for(int i = 0; i < n_pairs; i++) {
#     int site_one_index = pair_rows(i,0) - 1;
#     int site_two_index = pair_rows(i,1) - 1;
#     float n_spp_common = 0;
#     for(int i_spp_one=0; i_spp_one<site_rich[site_one_index]; i_spp_one++){
#       for(int i_spp_two=0; i_spp_two<site_rich[site_two_index]; i_spp_two++){
#         if(comp(site_one_index,i_spp_one) == comp(site_two_index,i_spp_two)){
#           n_spp_common += 1;
#           }
#         }
#       }
#     float sum_rich = site_rich[site_one_index] + site_rich[site_two_index];
#     out[i] = (1 - ((2 * n_spp_common) / (sum_rich)));
#     }
#   return out;
#   }')
# ##-------------------------------------------------------------------------------------------------------------##
#'@title Random site-pair sampler
#'
#'@description Sample site-pairs randomly.
#'
#'@param site.env.data (dataframe) A dataframe holding the location and environmental conditions for each site (grid cell) for which we have species composition data.
#'@param n.pairs.target (integer) The number of site-pairs to select.
#'@param output.folder (string) A folder to save the outputs to. If none specified, no file is written.
#'@param output.name (string) A name to use in saving the outputs. Default: 'pairs_table_dissim'.
#'@param verbose (boolean) Print messages to console. Default TRUE.
#'
#'@return Dataframe, site-pairs table, being first 6 columns of the GDM input table, with dissimilarities not calculated. Also includes four extra cols at the end, containing the decimal lat & long of both sites in the pair.
#'
#'@examples output = sitepair_sample_random(My.site.env.data, n.pairs.target=10000, output.folder = 'C:/Users/processed_data', output.name = 'My.sitepair.data')
#'
#'@importFrom matrixStats rowMins rowMaxs
#'
#'@export
sitepair_sample_random=function(site.env.data,
n.pairs.target,
output.folder = NULL,
output.name = "site_pairs_data",
verbose=FALSE)
{
# create a table to catch the row indices for the pairs selected for modelling
train.pairs<-matrix(c(-3,-2,-1,0), nrow=2, ncol=2)
colnames(train.pairs)<-c("temp.i", "temp.j")
# now loop until we have enough pairs randomly sampled
for(i in 1:1000)
{
# obtain row indices for randomly selected of pair-sites
vals <- sample.int(nrow(site.env.data), (n.pairs.target*2), replace=TRUE)
# format the random sample of site indices into pairs
ij.pairs<-cbind(vals[c(1:n.pairs.target)], vals[c((n.pairs.target+1):(n.pairs.target*2))])
# rearrange indices for each site pair so that the smallest site index comes first
temp.i<-rowMins(ij.pairs)
temp.j<-rowMaxs(ij.pairs)
ij.pairs<-cbind(temp.i,temp.j)
# omit duplicate site pairs within this sample (note: we wouldn't do this step under a bootstrapping approach)
ij.pairs<-unique(ij.pairs)
# and omit site pairs that have already been selected
ij.temp<-rbind(train.pairs, ij.pairs)
ij.temp<-unique(ij.temp)
selected.ij.pairs<-ij.temp[c((nrow(train.pairs)+1):nrow(ij.temp)),]
# and add these pairs to the main list of pairs selected for modelling
train.pairs<-rbind(train.pairs, selected.ij.pairs)
# check if we have enough pairs now (if so, randomly remove the necessary amount, so we hit our target,
# then break out of the loop)
if(nrow(train.pairs) >= (n.pairs.target + 2))
{
# remove the initial rows from train.pairs
train.pairs<-train.pairs[-c(1,2),]
# check how many excess pairs we have
n.excess <- nrow(train.pairs) - n.pairs.target
# Randomly select pairs to drop & remove them from the selected list
if(n.excess > 0)
{
drop.indices <- sample(seq_len(n.excess), size = n.excess, replace = FALSE)
train.pairs<-train.pairs[-drop.indices,]
}# end if n.excess > 0
# And break out of the loop
break()
}# end if nrow(train.pairs) >= (n.pairs.target + 2)
} # end for i
# Provide a warning if not enough pairs were selected
if(nrow(train.pairs) < n.pairs.target)
{
# remove the initial rows from train.pairs
train.pairs<-train.pairs[-c(1,2),]
warning("less pairs selected than desired", call. = FALSE)
}
# Prepare the start of a GDM input table for the pairs selected
Pairs.table <- data.frame(distance = 0,
weights = 1,
s1.xCoord = site.env.data$xCoord[train.pairs[,1]],
s1.yCoord = site.env.data$yCoord[train.pairs[,1]],
s2.xCoord = site.env.data$xCoord[train.pairs[,2]],
s2.yCoord = site.env.data$yCoord[train.pairs[,2]]
#s1.decimalLongitude = site.env.data$decimalLongitude[train.pairs[,1]],
#s1.decimalLatitude = site.env.data$decimalLatitude[train.pairs[,1]],
#s2.decimalLongitude = site.env.data$decimalLongitude[train.pairs[,2]],
#s2.decimalLatitude = site.env.data$decimalLatitude[train.pairs[,2]])
)
return(Pairs.table)
}# end sitepair.sample.random
##---------------------------------------------------------------------------------------##
##---------------------------------------------------------------------------------------##
library(lubridate)
seconds_to_period()
seconds_to_period
library(gdmEngine)
src = 'R:/DEV/biol/birds/bird-backbone-combined.csv'
bbone = read.csv(src, stringsAsFactors = FALSE)
dst = sprintf('%s/from-ala-2', dirname(src))
if(!dir.exists(dst)) dir.create(dst)
bbone = as.vector(bbone$x)
head(bbone)
install_github("cwarecsiro/gdmEngine/gdmEngine",
quick = TRUE, upgrade_dependencies = FALSE)
library(devtools)
install_github("cwarecsiro/gdmEngine/gdmEngine",
quick = TRUE, upgrade_dependencies = FALSE)
job_status
get_pid
library(devtools)
install_github("cwarecsiro/gdmEngine/gdmEngine",
quick = TRUE, upgrade_dependencies = FALSE)
job_status
download_taxalist
library(gdmEngine)
get_pid = function(){
now = grep("^rsession",readLines(textConnection(system('tasklist',intern=TRUE))),value=TRUE)
pids = NULL
for (i in now){
p = lapply(strsplit(i, ' ')[[1]], function(x)
Filter(haschar, x))
pids = c(pids, unlist(p)[2])
}
return(pids)
}
outersect = function(x, y){
sort(c(setdiff(x, y), setdiff(y, x)))
}
job_status = function(pid){
now = get_pid()
if(pid %in% now){
cat(sprintf('Process %s still running', pid), sep = '\n')
} else {
cat(sprintf('Process %s not running', pid), sep = '\n')
}
}
src = 'R:/DEV/biol/birds/bird-backbone-combined.csv'
bbone = read.csv(src, stringsAsFactors = FALSE)
dst = sprintf('%s/from-ala-2', dirname(src))
if(!dir.exists(dst)) dir.create(dst)
bbone = as.vector(bbone$x)
birds = download_taxalist(specieslist = bbone,
dst = dst,
parallel = TRUE,
background = TRUE)
library(devtools)
install_github("cwarecsiro/gdmEngine/gdmEngine",
quick = TRUE, upgrade_dependencies = FALSE)
library(gdmEngine)
job_status
src = 'R:/DEV/biol/birds/bird-backbone-combined.csv'
bbone = read.csv(src, stringsAsFactors = FALSE)
dst = sprintf('%s/from-ala-2', dirname(src))
if(!dir.exists(dst)) dir.create(dst)
bbone = as.vector(bbone$x)
birds = download_taxalist(specieslist = bbone,
dst = dst,
parallel = TRUE,
background = TRUE)
now = grep("^rsession",readLines(textConnection(system('tasklist',intern=TRUE))),value=TRUE)
now
i = 1
i = now[1]
i
strsplit(i, ' ')[[1]],
strsplit(i, ' ')[[1]]
hachar
haschar = ifelse(x == "", TRUE, FALSE)
haschar = function(x) ifelse(x == "", TRUE, FALSE)
pids = NULL
for (i in now){
#i = now[1]
p = lapply(strsplit(i, ' ')[[1]], function(x)
Filter(haschar, x))
pids = c(pids, unlist(p)[2])
}
pids
haschar = function(x) ifelse(x == "", FALSE, TRUE)
pids = NULL
for (i in now){
#i = now[1]
p = lapply(strsplit(i, ' ')[[1]], function(x)
Filter(haschar, x))
pids = c(pids, unlist(p)[2])
}
pids
library(devtools)
install_github("cwarecsiro/gdmEngine/gdmEngine",
quick = TRUE, upgrade_dependencies = FALSE)
library(gdmEngine)
birds = download_taxalist(specieslist = bbone,
dst = dst,
parallel = TRUE,
background = TRUE)
birds
download_taxalist
after = get_pids()
get_pids
job_status
get_pids = function(){
now = grep("^rsession",readLines(textConnection(system('tasklist',intern=TRUE))),value=TRUE)
pids = NULL
for (i in now){
p = lapply(strsplit(i, ' ')[[1]], function(x)
Filter(haschar, x))
pids = c(pids, unlist(p)[2])
}
return(pids)
}
haschar = function(x) ifelse(x == "", FALSE, TRUE)
outersect = function(x, y){
sort(c(setdiff(x, y), setdiff(y, x)))
}
#'@title Check status of background process
#'@export
job_status = function(pid){
now = get_pids()
if(pid %in% now){
cat(sprintf('Process %s still running', pid), sep = '\n')
} else {
cat(sprintf('Process %s not running', pid), sep = '\n')
}
}
get_pids()
library(devtools)
library(roxygen2)
library(Rcpp)
update_build = function(){
pkg_root = '//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/gdmEngine'
## write DESCRIPTION file
DESCRIPTION = c('Package: gdmEngine',
'Version: 0.01',
paste('Date:', Sys.Date()),
'Title: Workflow for GDM',
'Description: Functions used to develop GDMs',
paste('Author:', unname(Sys.info()['user'])),
'Maintainer: Chris Ware <chris.ware@csiro.au>',
'SystemRequirements: git with shell distribution',
'Licence: errr',
#paste('Authors@R:', unname(Sys.info()['user']))
'Imports:
assertthat,
DescTools,
gdm,
magrittr,
nnls,
matrixStats,
parallel,
plyr,
raster,
Rcpp,
sp,
spatstat,' #,
#'LinkingTo: Rcpp'
)
sink(paste(pkg_root, 'DESCRIPTION', sep = '/'))
cat(DESCRIPTION, sep = '\n')
sink()
## Build with devtools
setwd(pkg_root)
document()
build()
install(quick = TRUE)
}
update_build()
update_build = function(){
pkg_root = '//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/gdmEngine'
## write DESCRIPTION file
DESCRIPTION = c('Package: gdmEngine',
'Version: 0.01',
paste('Date:', Sys.Date()),
'Title: Workflow for GDM',
'Description: Functions used to develop GDMs',
paste('Author:', unname(Sys.info()['user'])),
'Maintainer: Chris Ware <chris.ware@csiro.au>',
'SystemRequirements: git with shell distribution',
'Licence: errr',
#paste('Authors@R:', unname(Sys.info()['user']))
'Imports:
assertthat,
DescTools,
gdm,
magrittr,
nnls,
matrixStats,
parallel,
plyr,
raster,
Rcpp,
sp,
spatstat,' #,
#'LinkingTo: Rcpp'
)
sink(paste(pkg_root, 'DESCRIPTION', sep = '/'))
cat(DESCRIPTION, sep = '\n')
sink()
## Build with devtools
setwd(pkg_root)
document()
build()
install(quick = TRUE, upgrade_dependencies = FALSE)
}
update_build()
update_build
