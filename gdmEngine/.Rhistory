n.preds<-n.preds-1
}else{ # then we must be dropping geo
geo<-FALSE
n.preds<-n.preds-1
}
}else{
in.vars.in[drop.var] <- 0
n.preds<-n.preds-1
} # end else
}# end if n.preds>n.predictors.min
out.col<-out.col+1
}# end for i.drp
###################################################################################
## Now we have a final model, run cross-validation with random test set         ##
if(n.preds <= n.predictors.min) # -- FINAL -- FINAL -- FINAL -- FINAL --
{
# Create a catcher for the final model
final.mod.MAE.set <- rep(0, times=n.crossvalid.tests)
final.mod.RMSE.set <- rep(0, times=n.crossvalid.tests)
final.mod.equRMSE.set <- rep(0, times=n.crossvalid.tests)
final.mod.D2.set <- rep(0, times=n.crossvalid.tests)
final.mod.MAE.rnd.set <- rep(0, times=n.crossvalid.tests)
final.mod.RMSE.rnd.set <- rep(0, times=n.crossvalid.tests)
final.mod.equRMSE.rnd.set <-rep(0, times=n.crossvalid.tests)
final.mod.D2.rnd.set <- rep(0, times=n.crossvalid.tests)
for(i.test in 1:n.crossvalid.tests)
{
# Grab the test and train site-pair data
Training.table.In <- train.lst[[i.test]]
Testing.table.In <- test.lst[[i.test]]
Testing.Rnd.table.In <- test.lst.rnd[[i.test]]
# Remove previously omitted variables
Training.table.In <- Training.table.In[,in.vars.cols]
Testing.table.In <- Testing.table.In[,in.vars.cols]
Testing.Rnd.table.In <- Testing.Rnd.table.In[,in.vars.cols]
# remove the rows with no data
Training.table.In<-Training.table.In[complete.cases(Training.table.In),]
Testing.table.In<-Testing.table.In[complete.cases(Testing.table.In),]
Testing.Rnd.table.In<-Testing.Rnd.table.In[complete.cases(Testing.Rnd.table.In),]
# For the applied sub-sampling scheme
validation.results.smp<- gdm_SingleCrossValidation(Training.table.In,
Testing.table.In,
geo=geo)
final.mod.MAE.set[i.test] <- validation.results.smp$Mean.Absolute.Error
final.mod.RMSE.set[i.test] <- validation.results.smp$Root.Mean.Squre.Error
final.mod.equRMSE.set[i.test] <- validation.results.smp$Equalised.RMSE
final.mod.D2.set[i.test] <- validation.results.smp$Test.Deviance.Explained
# For a purely random sample
validation.results.rnd<- gdm_SingleCrossValidation(Training.table.In,
Testing.Rnd.table.In,
geo=geo)
final.mod.MAE.rnd.set[i.test] <- validation.results.rnd$Mean.Absolute.Error
final.mod.RMSE.rnd.set[i.test] <- validation.results.rnd$Root.Mean.Squre.Error
final.mod.equRMSE.rnd.set[i.test] <- validation.results.rnd$Equalised.RMSE
final.mod.D2.rnd.set[i.test] <- validation.results.rnd$Test.Deviance.Explained
} # end for each i.test
} # end if n.preds <= n.predictors.min
###################################################################################
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
proc.time() - ptm
in.vars.in
in.vars
## Now we have a final set of predictors, fit a full model sampling site-pairs from
## the full set of sites ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
# Set up catching matrices for the model parameters
in.vars <- in.vars[which(in.vars.in > 0)]
in.vars
var.names <- var.names[which(in.vars.in > 0)]
var.names
n.params<-n.preds*3 # ASSUMING 3 KNOTS PER VARIABLE AT THE MOMENT
intercept.set<-rep(0, times=n.crossvalid.tests)
deviance.explained.set <- rep(0, times=n.crossvalid.tests)
final.mod.obs.dissim.evenness <- rep(0, times=n.crossvalid.tests)
coefficients.set<-matrix(0, nrow=n.crossvalid.tests, ncol=n.params)
knots.set<-matrix(0, nrow=n.crossvalid.tests, ncol=n.params)
final.mod.dissimilarity<-matrix(0, nrow=n.crossvalid.tests, ncol=6, dimnames=list(paste0("sample",c(1:n.crossvalid.tests)),c("Min.","1st Qu.","Median","Mean","3rd Qu.","Max.")))
i.test<-1
Pairs.Table.Train <- sitepair_sample_random(site.env.data = site.env.data,
n.pairs.target = n.pairs.train)
Pairs.Table.Train <- calculate_dissimilarities(pairs.table = Pairs.Table.Train,
composition.data = composition.data,
verbose=FALSE)
Pairs.Table.Train$s1.site.ID <- paste(Pairs.Table.Train$s1.decimalLongitude, Pairs.Table.Train$s1.decimalLatitude, sep = '_')
Pairs.Table.Train$s2.site.ID <- paste(Pairs.Table.Train$s2.decimalLongitude, Pairs.Table.Train$s2.decimalLatitude, sep = '_')
# Format these dataframes into GDM input tables
Training.table.In <- Pairs.Table.Train[,c(1:6)]
# Prepare predictor variables table
Training.GDM.input.vars <- matrix(0, nrow=nrow(Training.table.In), ncol=(length(in.vars)*2))
colnames(Training.GDM.input.vars) <- c(paste0("s1.",var.names),paste0("s2.",var.names))
summary(Training.GDM.input.vars)
for(i.var in 1:length(in.vars))
{
Training.GDM.input.vars[,i.var] <- site.env.data[match(as.character(Pairs.Table.Train$s1.site.ID), as.character(site.env.data$xy)), (n.cols.start+in.vars[i.var])]
Training.GDM.input.vars[,(length(in.vars)+i.var)] <- site.env.data[match(as.character(Pairs.Table.Train$s2.site.ID), as.character(site.env.data$xy)), (n.cols.start+in.vars[i.var])]
} # end for i.var
summary(Training.GDM.input.vars)
# Join the variables to the site-pair table
Training.table.In <- cbind(Training.table.In, Training.GDM.input.vars)
Training.table.In<-Training.table.In[complete.cases(Training.table.In),]
summary(Training.table.In)
library(gdm)
oldw <- getOption("warn")
options(warn = -1)
Final.mod <- gdm(Training.table.In,
geo=geo)
options(warn = oldw)
# Catch the parameters/stats
intercept.set[i.test] <- Final.mod$intercept
deviance.explained.set[i.test] <- Final.mod$explained
coefficients.set[i.test,] <- Final.mod$coefficients
knots.set[i.test,] <- Final.mod$knots
final.mod.dissimilarity[i.test,]<-summary(Training.table.In$distance)
dissim.hist <- hist(Training.table.In$distance, breaks=seq(from=0, to=1, by=0.025))
final.mod.obs.dissim.evenness[i.test] <- 1 - (DescTools::Gini(dissim.hist$counts))
Final.mod$explained
for(i.test in 2:n.crossvalid.tests)
{
## SUBSAMPLE SITE-PAIRS  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  ##
if(sample.method == 'random')
{
Pairs.Table.Train <- sitepair_sample_random(site.env.data = site.env.data,
n.pairs.target = n.pairs.train)
}#end if sample.method == 'random'
if(sample.method == 'geodist')
{
Pairs.Table.Train <- sitepair_sample_geographic(site.env.data = site.env.data,
n.pairs.target = n.pairs.train,
b.used.factor=b.used.factor,
b.dpair.factor=b.dpair.factor)
}#end if sample.method == 'geodist'
if(sample.method == 'envdist')
{
Pairs.Table.Train <- sitepair_sample_environment(site.env.data = site.env.data,
n.pairs.target = n.pairs.train,
b.used.factor=b.used.factor,
b.epair.factor=b.epair.factor)
}#end if sample.method == 'envdist'
if(sample.method == 'geodens')
{
Pairs.Table.Train <- sitepair_sample_density(site.env.data = site.env.data,
n.pairs.target = n.pairs.train,
domain.mask=domain.mask,
pcs.projargs=pcs.projargs,
b.used.factor=b.used.factor,
sigma.spair=sigma.spair,
b.spair.factor=spair.factor)
}#end if sample.method == 'geodens'
##  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  ##
Pairs.Table.Train <- calculate_dissimilarities(pairs.table = Pairs.Table.Train,
composition.data = composition.data,
verbose=FALSE)
Pairs.Table.Train$s1.site.ID <- paste(Pairs.Table.Train$s1.decimalLongitude, Pairs.Table.Train$s1.decimalLatitude, sep = '_')
Pairs.Table.Train$s2.site.ID <- paste(Pairs.Table.Train$s2.decimalLongitude, Pairs.Table.Train$s2.decimalLatitude, sep = '_')
# Format these dataframes into GDM input tables
Training.table.In <- Pairs.Table.Train[,c(1:6)]
# Prepare predictor variables table
Training.GDM.input.vars <- matrix(0, nrow=nrow(Training.table.In), ncol=(length(in.vars)*2))
colnames(Training.GDM.input.vars) <- c(paste0("s1.",var.names),paste0("s2.",var.names))
# catch the env data for both sites in the pair
for(i.var in 1:length(in.vars))
{
Training.GDM.input.vars[,i.var] <- site.env.data[match(as.character(Pairs.Table.Train$s1.site.ID), as.character(site.env.data$xy)), (n.cols.start+in.vars[i.var])]
Training.GDM.input.vars[,(length(in.vars)+i.var)] <- site.env.data[match(as.character(Pairs.Table.Train$s2.site.ID), as.character(site.env.data$xy)), (n.cols.start+in.vars[i.var])]
} # end for i.var
# Join the variables to the site-pair table
Training.table.In <- cbind(Training.table.In, Training.GDM.input.vars)
Training.table.In<-Training.table.In[complete.cases(Training.table.In),]
# Fit a GDM [without warnings]
oldw <- getOption("warn")
options(warn = -1)
Final.mod <- gdm(Training.table.In,
geo=geo)
options(warn = oldw)
# Catch the parameters/stats
intercept.set[i.test] <- Final.mod$intercept
deviance.explained.set[i.test] <- Final.mod$explained
coefficients.set[i.test,] <- Final.mod$coefficients
knots.set[i.test,] <- Final.mod$knots
final.mod.dissimilarity[i.test,]<-summary(Training.table.In$distance)
dissim.hist <- hist(Training.table.In$distance, breaks=seq(from=0, to=1, by=0.025),plot=FALSE)
final.mod.obs.dissim.evenness[i.test] <- 1 - (DescTools::Gini(dissim.hist$counts))
} # end for i.test
#replace the final model object data with the mean values across models
Final.mod$intercept<-mean(intercept.set)
Final.mod$explained<-mean(deviance.explained.set)
Final.mod$coefficients<-colMeans(coefficients.set)
Final.mod$knots<-colMeans(knots.set)
n.pairs.target
n.pairs.target = n.pairs.train
domain.mask
pcs.projargs
# ARGUMENTS ################################
# specify the distance from a sample point to use in skipping sample points (i.e. for sample points in the sea)
b.skip <- 3 # i.e. 3 times the bandwidth
# specify the distance between sample points, as a multiple of the bandwidth
inter.sample.pt.b.factor <- 2 # i.e. 2 x bandwidth between sample points
# specify the proportion of sites to add to the local sample that are drawn at random from the whole region
prop.sites.background <- 0.1 # i.e. 10% of sites should be a random sample across the whole region
is.null(pcs.projargs)
library(raster)
library(sp)
# Find the extent of the domain, in projected coordinates
pcs.domain.ext <- raster::projectExtent(domain.mask,
crs=sp::CRS(pcs.projargs))
pcs.domain.ext
# start of the geographic sample net
start.x <- pcs.domain.ext@extent@xmin + runif(1,0,(bandwidth/2))
is.null(bandwidth))
is.null(bandwidth)
bandwidth=NULL
is.null(bandwidth)
if(is.null(bandwidth))
{bandwidth <- ((pcs.domain.ext@extent@xmax - pcs.domain.ext@extent@xmin)/20)}
# start of the geographic sample net
start.x <- pcs.domain.ext@extent@xmin + runif(1,0,(bandwidth/2))
start.y <- pcs.domain.ext@extent@ymin + runif(1,0,(bandwidth/2))
b.skip <- 3
inter.sample.pt.b.factor <- 2
prop.sites.background <- 0.1
# The position of all x's & y's
n.x <- ceiling((pcs.domain.ext@extent@xmax - pcs.domain.ext@extent@xmin) / (inter.sample.pt.b.factor*bandwidth))
n.y <- ceiling((pcs.domain.ext@extent@ymax - pcs.domain.ext@extent@ymin) / (inter.sample.pt.b.factor*bandwidth))
# The position of all x's & y's
sample.pts <- expand.grid(x.pos = seq(from=start.x, by=(inter.sample.pt.b.factor*bandwidth), length.out = n.x),
y.pos = seq(from=start.y, by=(inter.sample.pt.b.factor*bandwidth), length.out = n.y))
plot(site.env.data$xCoord, site.env.data$yCoord)
points(sample.pts,col='red')
pcs.domain.ext@extent
n.x
n.y
# The position of all x's & y's [hexagonal net]
n.x1 <- ceiling((pcs.domain.ext@extent@xmax - start.x) / (inter.sample.pt.b.factor*bandwidth))
n.x1
n.x2 <- ceiling((pcs.domain.ext@extent@xmax - (start.x + ((inter.sample.pt.b.factor*bandwidth)/2))) / (inter.sample.pt.b.factor*bandwidth))
n.x2
(inter.sample.pt.b.factor*bandwidth)
y.interval <- sqrt(((inter.sample.pt.b.factor*bandwidth)^2)-(((inter.sample.pt.b.factor*bandwidth)/2)^2))
y.interval
n.y <- ceiling((pcs.domain.ext@extent@ymax - start.y) / y.interval)
n.y
n.x1
y.interval
n.y
n.y <- ceiling((pcs.domain.ext@extent@ymax - start.y) / y.interval)
n.y
# The position of all x's & y's [hexagonal net]
sample.pts.1 <- expand.grid(x.pos = seq(from=start.x, by=(inter.sample.pt.b.factor*bandwidth), length.out = n.x1),
y.pos = seq(from=start.y, by=(y.interval*2), length.out = ceiling(n.y/2)))
sample.pts.2 <- expand.grid(x.pos = seq(from=(start.x + ((inter.sample.pt.b.factor*bandwidth)/2)), by=(inter.sample.pt.b.factor*bandwidth), length.out = n.x2),
y.pos = seq(from=(start.y + y.interval), by=(y.interval*2), length.out = floor(n.y/2)))
sample.pts <- rbind(sample.pts.1,sample.pts.2)
x.c <- c(pcs.domain.ext@extent@xmin, pcs.domain.ext@extent@xmax, pcs.domain.ext@extent@xmin, pcs.domain.ext@extent@xmax)
y.c <- c(pcs.domain.ext@extent@ymin, pcs.domain.ext@extent@ymin, pcs.domain.ext@extent@ymax, pcs.domain.ext@extent@ymax)
cbind(x.c,y.c)
corners<- cbind(x.c,y.c)
plot(corners,col='blue')
points(site.env.data$xCoord, site.env.data$yCoord)
points(sample.pts,col='red')
start.x
pcs.domain.ext@extent@xmin
start.y
pcs.domain.ext@extent@ymin
summary(sample.pts)
pcs.domain.ext@extent
nrow(sample.pts)
sample.pts<- unique(sample.pts)
nrow(sample.pts)
###### Select pairs based on weighting at each sample point ############################################
# work out how many random samples to take at each sample point (assuming average weight for any grid cell = 1)
n.samples <- ceiling((n.pairs.target/nrow(site.env.data))*1.5) #multiply by 1.5 for safety buffer
n.samples
n.pairs.target/nrow(site.env.data
)
# set up a catcher for the sampled pairs
train.pairs<-NULL
i.pt<-1
nrow(site.env.data)
# determine the distance of each point with composition data from the sample point
#### NEW ####
## UP TO HERE
## ** test this
pairs.distance <- pts.euc.distance(x1=sample.pts[i.pt,1], y1=sample.pts[i.pt,2], x2=site.env.data[,3], y2=site.env.data[,4])
length(pairs.distance)
hist(pairs.distance)
#### OLD ####    dist.to.pt <- raster::pointDistance(p1=sample.pts[i.pt,], p2=site.env.data[,c(3:4)], lonlat=T)
site.wt <- exp(-0.5*((dist.to.pt/bandwidth)^2))
# determine the distance of each point with composition data from the sample point
#### NEW ####
dist.to.pt <- pts.euc.distance(x1=sample.pts[i.pt,1], y1=sample.pts[i.pt,2], x2=site.env.data[,3], y2=site.env.data[,4])
#### OLD ####    dist.to.pt <- raster::pointDistance(p1=sample.pts[i.pt,], p2=site.env.data[,c(3:4)], lonlat=T)
site.wt <- exp(-0.5*((dist.to.pt/bandwidth)^2))
max(site.wt) < (exp(-0.5*(((b.skip*bandwidth)/bandwidth)^2)))
i.pt<-50
plot(corners,col='blue')
points(sample.pts[50,])
points(sample.pts[30,])
points(sample.pts[28,])
i.pt<-28
# determine the distance of each point with composition data from the sample point
#### NEW ####
dist.to.pt <- pts.euc.distance(x1=sample.pts[i.pt,1], y1=sample.pts[i.pt,2], x2=site.env.data[,3], y2=site.env.data[,4])
#### OLD ####    dist.to.pt <- raster::pointDistance(p1=sample.pts[i.pt,], p2=site.env.data[,c(3:4)], lonlat=T)
site.wt <- exp(-0.5*((dist.to.pt/bandwidth)^2))
max(site.wt) < (exp(-0.5*(((b.skip*bandwidth)/bandwidth)^2)))
in.sites<-NULL
n.samples
i.sam<-1
# Use the site weights to randomly select sites
site.sample <- rbinom(n=length(site.wt), size=1, prob=site.wt)
in.sites<-c(in.sites, which(site.sample>0))
in.sites<-NULL
for(i.sam in 1:n.samples)
{
# Use the site weights to randomly select sites
site.sample <- rbinom(n=length(site.wt), size=1, prob=site.wt)
in.sites<-c(in.sites, which(site.sample>0))
}#end for i.sam
length(in.sites)>1
# Sample additional long-distance sites randomly from the full set
n.background.sites <- floor(length(in.sites) * prop.sites.background)
n.background.sites>0
add.sites<-sample.int(nrow(site.env.data), n.background.sites, replace=TRUE) # Note, probably better to sample off the other sample points, as this approach will be biased to heavily sampled areas
in.sites<-c(in.sites,add.sites)
# permute the order of sites in in.sites
in.sites<-sample(in.sites, replace = FALSE)
if(!(length(in.sites) %% 2 == 0 ))
{in.sites<-in.sites[-(length(in.sites))]}
n.pairs.sample<-length(in.sites)/2
ij.pairs<-cbind(in.sites[c(1:n.pairs.sample)], in.sites[c((n.pairs.sample+1):length(in.sites))])
# rearrange indices for each site pair so that the smalles comes first
temp.i<-matrixStats::rowMins(ij.pairs)
temp.j<-matrixStats::rowMaxs(ij.pairs)
ij.pairs<-cbind(temp.i,temp.j)
# remove any pairs composed of the same site
ij.pairs <- ij.pairs[(ij.pairs[,1] != ij.pairs[,2]),]
# omit duplicate site pairs within this sample (note: we wouldn't do this step under a bootstrapping approach)
ij.pairs<-unique(ij.pairs)
# and omit site pairs that have already been selected
train.pairs<-rbind(train.pairs, ij.pairs)
train.pairs<-unique(train.pairs)
train.pairs<-NULL
# Loop through the sample points
for(i.pt in 1:nrow(sample.pts))
{
# determine the distance of each point with composition data from the sample point
#### NEW ####
dist.to.pt <- pts.euc.distance(x1=sample.pts[i.pt,1], y1=sample.pts[i.pt,2], x2=site.env.data[,3], y2=site.env.data[,4])
#### OLD ####    dist.to.pt <- raster::pointDistance(p1=sample.pts[i.pt,], p2=site.env.data[,c(3:4)], lonlat=T)
site.wt <- exp(-0.5*((dist.to.pt/bandwidth)^2))
# if no sites are  within 3 bandwidths, skip this point
if(max(site.wt) < (exp(-0.5*(((b.skip*bandwidth)/bandwidth)^2))) ) #  w.ij <- exp(-0.5*((d.ij/bandwidth)^2))
{next}
in.sites<-NULL
for(i.sam in 1:n.samples)
{
# Use the site weights to randomly select sites
site.sample <- rbinom(n=length(site.wt), size=1, prob=site.wt)
in.sites<-c(in.sites, which(site.sample>0))
}#end for i.sam
# If we have sampled more than one site, then process the sites into pairs and add them to the master
# list of sampled site-pairs
if(length(in.sites)>1)
{
# Sample additional long-distance sites randomly from the full set
n.background.sites <- floor(length(in.sites) * prop.sites.background)
if(n.background.sites>0)
{
add.sites<-sample.int(nrow(site.env.data), n.background.sites, replace=TRUE) # Note, probably better to sample off the other sample points, as this approach will be biased to heavily sampled areas
in.sites<-c(in.sites,add.sites)
} # end if n.background.sites>0
# permute the order of sites in in.sites
in.sites<-sample(in.sites, replace = FALSE)
# Form the sites into pairs (remove the last site if there's an odd number)
if(!(length(in.sites) %% 2 == 0 ))
{in.sites<-in.sites[-(length(in.sites))]}
n.pairs.sample<-length(in.sites)/2
ij.pairs<-cbind(in.sites[c(1:n.pairs.sample)], in.sites[c((n.pairs.sample+1):length(in.sites))])
# rearrange indices for each site pair so that the smalles comes first
temp.i<-matrixStats::rowMins(ij.pairs)
temp.j<-matrixStats::rowMaxs(ij.pairs)
ij.pairs<-cbind(temp.i,temp.j)
# remove any pairs composed of the same site
ij.pairs <- ij.pairs[(ij.pairs[,1] != ij.pairs[,2]),]
# omit duplicate site pairs within this sample (note: we wouldn't do this step under a bootstrapping approach)
ij.pairs<-unique(ij.pairs)
# and omit site pairs that have already been selected
train.pairs<-rbind(train.pairs, ij.pairs)
train.pairs<-unique(train.pairs)
}# end if length(in.sites)>1
}# end for i.pt
nrow(train.pairs)>n.pairs.target
if(nrow(train.pairs)>n.pairs.target)
{train.pairs <- train.pairs[sample.int(nrow(train.pairs), n.pairs.target, replace=FALSE),]}
Pairs.table <- data.frame(distance	= 0,
weights = 1,
s1.xCoord = site.env.data$xCoord[train.pairs[,1]],
s1.yCoord = site.env.data$yCoord[train.pairs[,1]],
s2.xCoord = site.env.data$xCoord[train.pairs[,2]],
s2.yCoord = site.env.data$yCoord[train.pairs[,2]],
s1.decimalLongitude = site.env.data$decimalLongitude[train.pairs[,1]],
s1.decimalLatitude = site.env.data$decimalLatitude[train.pairs[,1]],
s2.decimalLongitude = site.env.data$decimalLongitude[train.pairs[,2]],
s2.decimalLatitude = site.env.data$decimalLatitude[train.pairs[,2]])
library(devtools)
library(roxygen2)
library(Rcpp)
pkg_root = '//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/gdmEngine'
## write DESCRIPTION file
DESCRIPTION = c('Package: gdmEngine',
'Version: 0.01',
paste('Date:', Sys.Date()),
'Title: Workflow for GDM',
'Description: Functions used to develop GDMs',
paste('Author:', unname(Sys.info()['user'])),
'Maintainer: Chris Ware <chris.ware@csiro.au>',
'SystemRequirements: git with shell distribution',
'Licence: errr',
#paste('Authors@R:', unname(Sys.info()['user']))
'Imports: Rcpp (>= 0.11.4)',
'LinkingTo: Rcpp'
)
sink(paste(pkg_root, 'DESCRIPTION', sep = '/'))
cat(DESCRIPTION, sep = '\n')
sink()
## Build with devtools
setwd(pkg_root)
document()
build()
install(quick = TRUE)
pkg_root = '//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/gdmEngine'
## write DESCRIPTION file
DESCRIPTION = c('Package: gdmEngine',
'Version: 0.01',
paste('Date:', Sys.Date()),
'Title: Workflow for GDM',
'Description: Functions used to develop GDMs',
paste('Author:', unname(Sys.info()['user'])),
'Maintainer: Chris Ware <chris.ware@csiro.au>',
'SystemRequirements: git with shell distribution',
'Licence: errr',
#paste('Authors@R:', unname(Sys.info()['user']))
'Imports: Rcpp (>= 0.11.4)',
'LinkingTo: Rcpp'
)
sink(paste(pkg_root, 'DESCRIPTION', sep = '/'))
cat(DESCRIPTION, sep = '\n')
sink()
## Build with devtools
setwd(pkg_root)
document()
build()
install(quick = TRUE)
gitr.push(files = 'all')
quantifies inequality, so high values are less even, zero = perfect evenness
dissim.summary[i.test,] <-
1
summary(Pairs.Table.Train$distance)
dissim.hist <- hist(Pairs.Table.Train$distance,
breaks = seq(from=0, to=1, by=0.025),
plot=FALSE)
## Number of times used
# Make a list of all the sites used, and find out how many times for each
sites.used <- c(Pairs.Table.Train$s1.site.ID, Pairs.Table.Train$s2.site.ID)
sites.times.used <- table(sites.used)
sites.times.used[c(1:5)]
sites.times.used[c(1:5),]
hist(sites.times.used)
# and add any sites that weren't used, with zeros
sites.used.names<-dimnames(sites.times.used)
zz<- sites.used.names %in% site.env.data$xy
zz<-sites.used.names %in% as.character(site.env.data$xy)
source('//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/sandpit/sitepair_sample_assessor.R', echo=TRUE)
# and add any sites that weren't used, with zeros
sites.used.names <- dimnames(sites.times.used)
# and add any sites that weren't used, with zeros
sites.used.names <- as.vector(dimnames(sites.times.used))
zz<- sites.used.names$sites.used %in% as.character(site.env.data$xy)
# and add any sites that weren't used, with zeros
sites.used.names <- dimnames(sites.times.used)
sites.used.names <- sites.used.names$sites.used
zz<- sites.used.names %in% as.character(site.env.data$xy)
sites.used.names <- sites.used.names[c(1:16000)]
zz<- sites.used.names %in% as.character(site.env.data$xy)
zz<- as.character(site.env.data$xy) %in% sites.used.names
sum(zz)
out.sites <- site.env.data$xy[as.character(site.env.data$xy) %in% sites.used.names]
out.sites <- site.env.data$xy[!(as.character(site.env.data$xy) %in% sites.used.names)]
length(sites.times.used)
length(out.sites)
as.character(out.sites[c(1:5)])
out.sites <- data.frame(rep(0, times=length(out.sites)),dimnames=as.character(out.sites))
summary(out.sites)
out.sites <- site.env.data$xy[!(as.character(site.env.data$xy) %in% sites.used.names)]
out.sites <- data.frame(rep(0, times=length(out.sites)), row.names=as.character(out.sites))
out.sites[c(1:5)]
out.sites[c(1:5),]
sites.times.used[c(1:5)]
zz<-data.frame('xy'=dimnames(sites.times.used),
'times.used'=as.numeric(sites.times.used))
zz[c(1:5),]
zz<-data.frame('xy'=as.character(dimnames(sites.times.used)),
'times.used'=as.numeric(sites.times.used))
zz[c(1:5),]
sites.times.used[c(1:5)]
sites.times.used$sites.used[c(1:5)]
sites.times.used2 <- as.data.frame(sites.times.used$sites.used)
sites.times.used2 <- as.data.frame(sites.times.used)
nrow(sites.times.used2)
ncol(sites.times.used2)
sites.times.used2[c(1:5),]
sites.times.used <- as.data.frame(sites.times.used)
sites.times.used$sites.used <- as.character(sites.times.used$sites.used)
