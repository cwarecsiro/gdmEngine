download_args = list(
method = "indexed",
download_reason_id = 7,
output.folder = raw_files,
read = FALSE,
verbose = FALSE)
## default args
download_args = list(
method = "indexed",
download_reason_id = 7,
output.folder = raw_files,
read = FALSE,
verbose = FALSE)
user_args = list(...)
if (length(
)
)
)
user_args = list(...)
length(user_args)
log = list()
log$ALA_DOWNLOAD = Sys.Date()
log_f = paste0(output.folder, 'ALA_download_R_log_', Sys.Date(), '.RData')
save(log, file = log_f)
parallel
load(log_f)
not_done <- outersect(specieslist, names(log)[-1])
length(not_done)
spp<-1
counter(msg = 'Searching the ALA for records of', iterable = spp)
check <- tryCatch(ala <- do.call(download_occurrences,
c(list(taxon = spp), download_args)),
error = function(e) e)
for(spp in not_done){
## counter to print to console
counter(msg = 'Searching the ALA for records of', iterable = spp)
## Download records from ALA
## Log any errors in downloads by catching them and passing them
## through to the log.
check <- tryCatch(ala <- do.call(download_occurrences,
c(list(taxon = spp), download_args)),
error = function(e) e)
## log
if(!is.null(check)){ ## signifies error
log[[spp]] = check
} else {
log[[spp]] = 'Successfully downloaded file'
}
## Give the ALA a break
## Sys.sleep(5)
} # end spp loop
spp in not_done
download_args
not_done
do.call(download_occurrences,
c(list(taxon = spp), download_args)
download_occurrences(not_done, )
download_occurrences <- function (taxon, wkt, fields, qa, method = "offline",
email, download_reason_id, dwcHeaders = TRUE, dst = NULL, read = TRUE,
dry_run = FALSE, verbose = TRUE) {
pkg_check = suppressWarnings(lapply(c('httr', 'assertthat'), require, character.only = TRUE))
if(!all(unlist(pkg_check))) stop('Could not load one of required packages httr or asserthat')
## possibly leave this in case smaller downloads are required
method <- match.arg(tolower(method), c("offline", "indexed"))
if (method == "indexed") {
valid_fields_type <- "occurrence_stored"
#stop('indexed currently not available: use ALA4R package instead')
} else {
valid_fields_type <- "occurrence"
}
is.notempty.string = function(x) {
is.string(x) && !is.na(x) && nchar(x)>0
}
## query container
this_query <- list()
## must be included
if (!missing(taxon)) {
if (is.factor(taxon)) {
taxon <- as.character(taxon)
}
assert_that(is.notempty.string(taxon))
this_query$q <- taxon
}
# optional
if (!missing(wkt)) {
assert_that(is.notempty.string(wkt))
this_query$wkt <- wkt
}
## ammend if I leave out the above
if (length(this_query) == 0) {
stop("invalid request: need at least one of taxon, fq, or wkt to be specified")
}
## leave for now
if (method == "offline") {
#if (record_count_only)
#    stop("record_count_only can only be used with method=\"indexed\"")
if (missing(email) || !is.string(email) || nchar(email) < 1)
stop("email is required for method=offline")
}
## here I think I can just assume that a numeric reason is always entered.
## could even hard-code one.
reason_ok <- !is.na(download_reason_id)
if (reason_ok) {
## Should take a code from 0-12
reason_ok = reason_ok %in% seq(0:12)
}
## modify later
if (!reason_ok) {
stop("download_reason_id must be a valid reason_id. See...")
}
## This guy. I think for our purposes we just need a standard set of
## field names. These could be called from a file, or loaded in the
## function. Assume for now it is loaded into memory and called by
## fields arg.
if (!missing(fields)) {
assert_that(is.character(fields))
this_query$fields <- paste(fields, collapse = ",")
} else {
if(verbose) cat('... Requesting default fields', sep = '\n')
fields = as.character(c('occurrenceID',
'kingdom.p',
'phylum.p',
'classs.p',
'order.p',
'family.p',
'genus.p',
'subgenus.p',
'specificEpithet',
'scientificName.p',
'acceptedNameUsage',
'taxonConceptID.p',
'taxonRank.p',
'eventDate.p',
'year.p',
'decimalLatitude.p',
'decimalLongitude.p',
'coordinateUncertaintyInMeters.p'))
this_query$fields <- paste(fields, collapse = ",")
}
## Think this should error if missing. As above it will probably be a file
## that needs to be in memory (or set to a hard-coded path).
if (!missing(qa)){
assert_that(is.character(qa))
this_query$qa <- paste(qa, collapse = ",")
} else {
if(verbose) cat('... Requesting default quality assertion fields', sep = '\n')
qa = as.character(c('zeroCoordinates',
#'nameNotSupplied',
'nameNotRecognised',
'decimalLatLongCalculationFromEastingNorthingFailed',
'zeroLatitude',
#'decimalLatLongCalculationFromVerbatimFailed',
'coordinatesCentreOfCountry',
'processingError',
'occCultivatedEscapee',
'coordinatesCentreOfStateProvince',
'decimalLatLongConversionFailed',
'zeroLongitude'))
qa = 'includeall'
this_query$qa <- paste(qa, collapse = ",")
}
if(dwcHeaders){
this_query$dwcHeaders = 'true'
} else {
this_query$dwcHeaders = 'false'
}
if (method == "offline") this_query$email <- email
this_query$reasonTypeId <- download_reason_id
#this_query$sourceTypeId <- ala_sourcetypeid()
this_query$esc <- "\\"
## This guy!
#this_query$sep <- "\t"
this_query$sep <- ","
## Make the file name something intuitive
#this_query$file <- "data"
## Build file name from taxon search and date
fn = gsub("[[:punct:]]", "_", taxon)
fn = gsub("[[:punct:]]", "_", fn)
fn = paste(fn, Sys.Date(), sep = '_')
fn = gsub(' ', '_', fn)
this_query$file <- fn
## url builder (source utilities_internal.R)
build_url_from_parts <- function(base_url,path=NULL,query=list()) {
this_url <- parse_url(base_url)
this_url$path <- clean_path(this_url$path,path)
if (length(query)>0) {
this_url$query <- query
}
build_url(this_url)
}
## internal url builder function (source utilities_internal.R)
clean_path <- function(...,sep="/") {
path1 <- sapply(list(...),FUN=function(z)paste(z,sep=sep,collapse=sep)) ## collapse individual arguments
## workaround to avoid replacing "http://" with "http:/", since this is now used in GUID strings (July 2016)
path <- paste(path1,sep=sep,collapse=sep) ## paste parts together
path <- gsub("http://","http:@@",path,fixed=TRUE)
path <- gsub(paste0("[",sep,"]+"),sep,path) ## remove multiple slashes
path <- gsub("http:@@","http://",path,fixed=TRUE)
sub(paste0("^",sep),"",path) ## remove leading slash
}
## assign this directly
base_url = 'http://biocache.ala.org.au/ws/'
# getOption("ALA4R_server_config")$base_url_biocache
## I think this is all good.
## If I want to simply, remove 'indexed' option
if (method == "indexed"){
this_url <- build_url_from_parts(base_url, c("occurrences", "index", "download"),
query = this_query)
} else {
this_url <- build_url_from_parts(base_url, c("occurrences", "offline", "download"),
query = this_query)
}
if(dry_run) {
return(this_url)
} else {
## Build in a pinger to the ALA server.
if (method == "offline") {
## send request
result = GET(this_url)
if(verbose){
if(status_code(result) == 200) {
msg = 'ALA query is being processed'
fmat = paste(rep('-', nchar(msg)), collapse = '')
requestStatus = paste('Request status: ', content(result)$status, sep = '')
queueSize = paste('Queue size....: ', content(result)$queueSize, sep = '')
statusCheck = paste('Status check..: ', content(result)$statusUrl, sep = '')
cat('', fmat, msg, fmat, requestStatus, queueSize, statusCheck, sep = '\n')
shell.exec(paste(content(result)$statusUrl))
} else {
msg = paste(': Something has gone wrong with the request ',
'(status ', status_code(result), ')', sep = '')
errorType = content(result)$errorType
cat(errorType, msg, sep = '')
}
}
} else {
if(is.null(dst)){
tmp = tempfile()
response = GET(this_url, write_disk(tmp, overwrite=TRUE))
if(response$status_code != 200) cat(response$status_code)
dump_into = tempdir()
## assumes data.csv is always first...
df <- unzip(tmp, exdir = dump_into)[1] %>% read.csv(stringsAsFactors=FALSE)
return(df)
} else {
dst = check_filepath(dst)
dst = paste0(dst, this_query$file, '.zip')
response = GET(this_url, write_disk(dst, overwrite=TRUE))
if(response$status_code != 200) cat(response$status_code)
if(read){
dump_into = tempdir()
## assumes data.csv is always first...
df <- unzip(dst, exdir = dump_into)[1] %>% read.csv(stringsAsFactors=FALSE)
return(df)
}
}
}
}
}
zz <- download_occurrences(taxon = "Adelotus brevis", method="indexed",download_reason_id="7")
check <- tryCatch(ala <- do.call(download_occurrences,
c(list(taxon = spp), download_args)),
error = function(e) e)
load(log_f)
not_done <- outersect(specieslist, names(log)[-1])
for(spp in not_done){
## counter to print to console
counter(msg = 'Searching the ALA for records of', iterable = spp)
## Download records from ALA
## Log any errors in downloads by catching them and passing them
## through to the log.
check <- tryCatch(ala <- do.call(download_occurrences,
c(list(taxon = spp), download_args)),
error = function(e) e)
## log
if(!is.null(check)){ ## signifies error
log[[spp]] = check
} else {
log[[spp]] = 'Successfully downloaded file'
}
## Give the ALA a break
## Sys.sleep(5)
} # end spp loop
download_args
output.folder
check_filepath = function(filepath){
## check for trailing forwardslash and backslashes
check_nchar = substr(filepath, nchar(filepath), nchar(filepath))
if(check_nchar == '/'){
return(filepath)
} else if(check_nchar == '\\') {
return(filepath)
} else {
return(paste0(filepath, '/'))
}
}
output.folder = check_filepath(output.folder)
output.folder
raw_files = paste0(output.folder , 'raw_files')
dir.exists(raw_files)
raw_files
output.folder = species.records.folder
output.folder
raw_files = paste0(output.folder)# , 'raw_files')
raw_files
dir.exists(raw_files)
cat('Warning: output.folder exists and contents will be written over...')
download_args = list(
method = "indexed",
download_reason_id = 7,
output.folder = raw_files,
read = FALSE,
verbose = FALSE)
download_args
paste0(output.folder, 'ALA_download_R_log_', Sys.Date(), '.RData')
filepath(output.folder, paste0('ALA_download_R_log_', Sys.Date(), '.RData'))
file.path(output.folder, paste0('ALA_download_R_log_', Sys.Date(), '.RData'))
pkg_root = '//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/gdmEngine'
## write DESCRIPTION file
DESCRIPTION = c('Package: gdmEngine',
'Version: 0.01',
paste('Date:', Sys.Date()),
'Title: Workflow for GDM',
'Description: Functions used to develop GDMs',
paste('Author:', unname(Sys.info()['user'])),
'Maintainer: Chris Ware <chris.ware@csiro.au>',
'SystemRequirements: git with shell distribution'
#paste('Authors@R:', unname(Sys.info()['user']))
)
sink(paste(pkg_root, 'DESCRIPTION', sep = '/'))
cat(DESCRIPTION, sep = '\n')
sink()
## Build with devtools
setwd(pkg_root)
document()
build()
install()
download_taxalist(specieslist = species.names,
output.folder = species.records.folder)
specieslist = species.names
output.folder = species.records.folder
parallel = FALSE
if(!is.null(output.folder)){
if(!dir.exists(output.folder)) dir.create(output.folder)
} else {
output.folder = tempdir()
}# end else !is.null...
## in case the list is derived from a data.frame
if(is(specieslist, 'factor')){
specieslist = as.character(paste(specieslist))
}# end if is(specieslist...
## config output.folder for raw downloads
raw_files = paste0(output.folder)# , 'raw_files')
if(dir.exists(raw_files)){
cat('Warning: output.folder exists and contents will be written over...')
} else {
dir.create(raw_files)
}#end else dir.exists...
## default args
download_args = list(
method = "indexed",
download_reason_id = 7,
output.folder = raw_files,
read = FALSE,
verbose = FALSE)
download_args
log = list()
log$ALA_DOWNLOAD = Sys.Date()
log_f = file.path(output.folder, paste0('ALA_download_R_log_', Sys.Date(), '.RData'))
save(log, file = log_f)
log_f
log
## run
try(repeat{
load(log_f)
not_done <- outersect(specieslist, names(log)[-1])
## Break check
if(length(not_done)==0) break
## Not finished...? Start looping again.
if(parallel){
no_cores = parallel::detectCores() - 2
cl = parallel::makeCluster(no_cores)
clusterExport(cl, list(), envir=environment())
clusterExport(cl, list('download_args', 'download_occurrences',
'check_filepath', 'counter', 'outersect'))
#clusterEvalQ(cl, library(ALA4R))
clusterEvalQ(cl, library(gdmEngine))
cat(paste0('Searching the ALA for records of ', length(not_done),
' taxa...'))
check = parLapply(cl, not_done, function(x){
tryCatch({
do.call(download_occurrences,
c(list(taxon = x), download_args))
},
error = function(e) {
e
})
})
## log
check = lapply(check, function(x)
if(is.null(x)) x = 'Successfully downloaded file')
log = c(log, check)
names(log)[2:length(log)] = not_done
gc()
stopCluster(cl = cl)
} else { # end if parallel
## loop
for(spp in not_done){
## counter to print to console
counter(msg = 'Searching the ALA for records of', iterable = spp)
## Download records from ALA
## Log any errors in downloads by catching them and passing them
## through to the log.
check <- tryCatch(ala <- do.call(download_occurrences,
c(list(taxon = spp), download_args)),
error = function(e) e)
## log
if(!is.null(check)){ ## signifies error
log[[spp]] = check
} else {
log[[spp]] = 'Successfully downloaded file'
}
## Give the ALA a break
## Sys.sleep(5)
} # end spp loop
gc()
} # end else serial
## save log
save(log, file = log_f)
}) #end try(repeat...
log[[1]]
log[[2]]
load(log_f)
log_f
not_done <- outersect(specieslist, names(log)[-1])
not_done
log = list()
log$ALA_DOWNLOAD = Sys.Date()
log_f = file.path(output.folder, paste0('ALA_download_R_log_', Sys.Date(), '.RData'))
save(log, file = log_f)
load(log_f)
not_done <- outersect(specieslist, names(log)[-1])
length(not_done)==0
download_arg
download_args
for(spp in not_done){
## counter to print to console
counter(msg = 'Searching the ALA for records of', iterable = spp)
## Download records from ALA
## Log any errors in downloads by catching them and passing them
## through to the log.
check <- tryCatch(ala <- do.call(download_occurrences,
c(list(taxon = spp), download_args)),
error = function(e) e)
## log
if(!is.null(check)){ ## signifies error
log[[spp]] = check
} else {
log[[spp]] = 'Successfully downloaded file'
}
## Give the ALA a break
## Sys.sleep(5)
} # end spp loop
log[2]
log[2]$`Adelotus brevis`$message
counter = function(msg = 'Doing loop ', vec = NULL, iterable = NULL){
if(!exists('counter_idx')) counter_idx <<- 1
if(!is.null(vec)){
msg = paste(msg, vec[counter_idx])
} else if(!is.null(iterable)){
msg = paste0(msg, ' ', iterable)
} else {
msg = paste0(msg, counter_idx)
}
cat("\r", msg)
counter_idx <<- counter_idx + 1
Sys.sleep(0.2)
}
outersect = function(x, y) {
sort(c(setdiff(x, y), setdiff(y, x)))
}
unzipper = function(fn, memory_only = TRUE){
temp_dump = paste(dirname(fn), gsub('.zip', '', basename(fn)), sep = '/')
dir.create(temp_dump)
df = tryCatch(unzip(fn, exdir = temp_dump)[1] %>%
read.csv(stringsAsFactors=FALSE),
error = function(e) e)
if(memory_only) unlink(temp_dump, recursive = TRUE)
return(df)
}
check_filepath = function(filepath){
check_nchar = substr(filepath, nchar(filepath), nchar(filepath))
if(check_nchar == '/'){
return(filepath)
} else if(check_nchar == '\\\\') {
return(filepath)
} else {
return(paste0(filepath, '/'))
}
tempdir()
)
check_filepath = function(filepath){
## check for trailing forwardslash and backslashes
check_nchar = substr(filepath, nchar(filepath), nchar(filepath))
if(check_nchar == '/'){
return(filepath)
} else if(check_nchar == '\\') {
return(filepath)
} else {
return(paste0(filepath, '/'))
}
}
