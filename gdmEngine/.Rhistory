##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
proc.time() - ptm
ptm <- proc.time()
## Select candidate variables for modelling~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
# First assess correlation between variables
var.cor<-cor(site.env.data[,c((n.cols.start+1):ncol(site.env.data))], use="pairwise.complete.obs",method="pearson")
# And now combine the model individual variable explanatory power with the level of
# corellation between variables to select a set for use in an initial multivariate model.
# Set up the variable assessment
var.impt <- ind.var.test.stats.sampairs[,4]
var.impt[ind.var.test.stats.sampairs[,4] < Indiv.Dev.Explained.Min] <- -9999
var.impt <- var.impt[-length(var.impt)] #remove geographic distance
in.var.lst <- c()
i.var<-1
while(i.var<=n.vars)
{
nxt.bst.env<-as.numeric(which.max(var.impt))
if(as.logical(var.impt[nxt.bst.env]<0))
{break}
# check if this variable is too correlated to variables already selected
if(length(in.var.lst)>0)
{
cor.to.in.vars<-var.cor[c(in.var.lst),nxt.bst.env]
if(abs(max(cor.to.in.vars)) > correlation.threshold)
{
var.impt[nxt.bst.env] <- -9999
}# end if(max(cor.to.in.vars) > correlation.threshold)
if(abs(max(cor.to.in.vars)) <= correlation.threshold)
{
in.var.lst<-c(in.var.lst,nxt.bst.env)
var.impt[nxt.bst.env] <- -9999
} # end if(max(cor.to.in.vars) <= correlation.threshold)
} # end if length(in.var.lst)>0
if(length(in.var.lst)<1)
{
# catch the variable index and set its error to extreme (9999)
in.var.lst<-c(in.var.lst,nxt.bst.env)
var.impt[nxt.bst.env] <- -9999
} # end if length(in.var.lst)<1
i.var <- i.var + 1
} # end while i.var<n.env.variables
# Check if geographic distance should be used too, based on it's individual deviance explained
if(geo){
if(ind.var.test.stats.sampairs[nrow(ind.var.test.stats.sampairs),4] < Indiv.Dev.Explained.Min)
{
geo<-FALSE
} # end if(ind.var.test.stats.sampairs...
}# end if(geo)
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
proc.time() - ptm
ptm <- proc.time()
##NEW  ## Now run a Backward elimination variable selection routine based ~~~~~~~~~~~~~~~##
## on performance under cross-validation. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
in.vars <- in.var.lst
in.vars.in <- rep(1,times=length(in.vars))
# get the starting number of predictors
if(geo){
n.preds <- length(in.vars) + 1
}else{
n.preds <- length(in.vars)
}
# Check we have more than the minimum number of predictors to choose from. Otherwise just use all the predictors
if(n.preds > n.predictors.min)
{
drop.sequence <-c(n.preds:n.predictors.min)
}else{
drop.sequence <- n.preds
}
# Set up GDM input tables for each of the cross-validation sets#####################
var.names <- c(colnames(site.env.data[,c(n.cols.start + in.vars)]))
for(i.test in 1:n.crossvalid.tests)
{
# Grab the test and train site-pair data
Pairs.Table.Train <- train.lst[[i.test]]
Pairs.Table.Test <- test.lst[[i.test]]
Pairs.Table.Test.Rnd <- test.lst.rnd[[i.test]]
# Format these dataframes into GDM input tables
Training.table.In <- Pairs.Table.Train[,c(1:6)]
Testing.table.In <- Pairs.Table.Test[,c(1:6)]
Testing.Rnd.table.In <- Pairs.Table.Test.Rnd[,c(1:6)]
# Prepare predictor variables table
Training.GDM.input.vars <- matrix(0, nrow=nrow(Training.table.In), ncol=(length(in.vars)*2))
Testing.GDM.input.vars <- matrix(0, nrow=nrow(Testing.table.In), ncol=(length(in.vars)*2))
Testing.Rnd.GDM.input.vars <- matrix(0, nrow=nrow(Testing.Rnd.table.In), ncol=(length(in.vars)*2))
colnames(Training.GDM.input.vars) <- c(paste0("s1.",var.names),paste0("s2.",var.names))
colnames(Testing.GDM.input.vars) <- c(paste0("s1.",var.names),paste0("s2.",var.names))
colnames(Testing.Rnd.GDM.input.vars) <- c(paste0("s1.",var.names),paste0("s2.",var.names))
# find the row indices in env.data for the sites in the pairs table
s1.row.indices.train <- match(as.character(Pairs.Table.Train$s1.site.ID), as.character(site.env.data$xy))
s2.row.indices.train <- match(as.character(Pairs.Table.Train$s2.site.ID), as.character(site.env.data$xy))
s1.row.indices.test <- match(as.character(Pairs.Table.Test$s1.site.ID), as.character(site.env.data$xy))
s2.row.indices.test <- match(as.character(Pairs.Table.Test$s2.site.ID), as.character(site.env.data$xy))
s1.row.indices.test.rnd <- match(as.character(Pairs.Table.Test.Rnd$s1.site.ID), as.character(site.env.data$xy))
s2.row.indices.test.rnd <- match(as.character(Pairs.Table.Test.Rnd$s2.site.ID), as.character(site.env.data$xy))
# catch the env data for both sites in the pair
for(i.var in 1:length(in.vars))
{
# TRAINING
Training.GDM.input.vars[,i.var] <- site.env.data[s1.row.indices.train, (n.cols.start+in.vars[i.var])]
Training.GDM.input.vars[,(length(in.vars)+i.var)] <- site.env.data[s2.row.indices.train, (n.cols.start+in.vars[i.var])]
# TESTING
Testing.GDM.input.vars[,i.var] <- site.env.data[s1.row.indices.test, (n.cols.start+in.vars[i.var])]
Testing.GDM.input.vars[,(length(in.vars)+i.var)] <- site.env.data[s2.row.indices.test, (n.cols.start+in.vars[i.var])]
# RANDOM TESTING
Testing.Rnd.GDM.input.vars[,i.var] <- site.env.data[s1.row.indices.test.rnd, (n.cols.start+in.vars[i.var])]
Testing.Rnd.GDM.input.vars[,(length(in.vars)+i.var)] <- site.env.data[s2.row.indices.test.rnd, (n.cols.start+in.vars[i.var])]
} # end for i.var
# Join the variables to the site-pair table
Training.table.In <- cbind(Training.table.In, Training.GDM.input.vars)
Testing.table.In <- cbind(Testing.table.In, Testing.GDM.input.vars)
Testing.Rnd.table.In <- cbind(Testing.Rnd.table.In, Testing.Rnd.GDM.input.vars)
## PUT THE TEST AND TRAINING TABLES IN THE LISTS ## This replaces the first 6 cols with the full GDM tables
train.name <- paste('PairsTableTrain_',i.test, sep='')
test.name <- paste('PairsTableTest_',i.test, sep='')
test.name.rnd <- paste('PairsTableTestRnd_',i.test, sep='')
train.lst[[train.name]] <- Training.table.In
test.lst[[test.name]] <- Testing.table.In
test.lst.rnd[[test.name.rnd]] <- Testing.Rnd.table.In
}# end for i.test
####### DATA IS PREPPED, NOW BACKWARD VAR SELECTION
# Set up the output catcher for the backward elimination
if(geo){
drop.names <- c(var.names,"Geographic")
}else{
drop.names <- c(var.names)
}# end else geo
drop.stats.D2 <- matrix(nrow=length(drop.names),ncol=length(drop.sequence),dimnames=list(drop.names,paste0(drop.sequence,"_variables")) )
drop.stats.RMSE <- matrix(nrow=length(drop.names),ncol=length(drop.sequence),dimnames=list(drop.names,paste0(drop.sequence,"_variables")) )
drop.stats.eRMSE <- matrix(nrow=length(drop.names),ncol=length(drop.sequence),dimnames=list(drop.names,paste0(drop.sequence,"_variables")) )
each.droplevel.D2 <- matrix(0,nrow=(n.crossvalid.tests+1),ncol=length(drop.sequence),dimnames=list(c(c(1:n.crossvalid.tests),"mean"),paste0(drop.sequence,"_variables")) )
# Loop backwards, dropping the worst predictor
out.col<-1
for(i.drp in drop.sequence)
{
var.index.vars.in <- which(in.vars.in > 0)
n.vars.in <- sum(in.vars.in)
in.vars.cols <- c(1:6, (6+var.index.vars.in), (6+length(in.vars.in)+var.index.vars.in) )
drop.stats.D2[var.index.vars.in,out.col] <- 0
drop.stats.RMSE[var.index.vars.in,out.col] <- 0
drop.stats.eRMSE[var.index.vars.in,out.col] <- 0
if(geo)
{
drop.stats.D2[nrow(drop.stats.D2),out.col] <- 0
drop.stats.RMSE[nrow(drop.stats.RMSE),out.col] <- 0
drop.stats.eRMSE[nrow(drop.stats.eRMSE),out.col] <- 0
}#end if geo
# Loop through the cross-validation data sets
for(i.test in 1:n.crossvalid.tests)
{
# Grab the test and train site-pair data
Training.table.In <- train.lst[[i.test]]
Testing.table.In <- test.lst[[i.test]]
# Remove previously omitted variables
Training.table.In <- Training.table.In[,in.vars.cols]
Testing.table.In <- Testing.table.In[,in.vars.cols]
# remove the rows with no data
Training.table.In<-Training.table.In[complete.cases(Training.table.In),]
Testing.table.In<-Testing.table.In[complete.cases(Testing.table.In),]
# see how much deviance the full model explains
validation.results <- gdm_SingleCrossValidation(Training.table.In,
Testing.table.In,
geo=geo)
each.droplevel.D2[i.test,out.col] <- validation.results$Test.Deviance.Explained
# Drop each variable, and see how the error changes
for(i.var in 1:n.vars.in)
{
#drop i.var
p.Training.table.In <- Training.table.In[,-c((6+i.var),(6+n.vars.in+i.var))]
p.Testing.table.In <- Testing.table.In[,-c((6+i.var),(6+n.vars.in+i.var))]
# Test the reduced model against the testing data
validation.results <- gdm_SingleCrossValidation(p.Training.table.In,
p.Testing.table.In,
geo=geo)
drop.stats.D2[var.index.vars.in[i.var],out.col] <- drop.stats.D2[var.index.vars.in[i.var],out.col] + validation.results$Test.Deviance.Explained
drop.stats.RMSE[var.index.vars.in[i.var],out.col] <- drop.stats.RMSE[var.index.vars.in[i.var],out.col] + validation.results$Root.Mean.Squre.Error
drop.stats.eRMSE[var.index.vars.in[i.var],out.col] <- drop.stats.eRMSE[var.index.vars.in[i.var],out.col] + validation.results$Equalised.RMSE
} #end for i.var
# If geographic distance is still in, drop it to see the effect
if(geo)
{
validation.results <- gdm_SingleCrossValidation(Training.table.In,
Testing.table.In,
geo=FALSE)
drop.stats.D2[nrow(drop.stats.D2),out.col] <- drop.stats.D2[nrow(drop.stats.D2),out.col] + validation.results$Test.Deviance.Explained
drop.stats.RMSE[nrow(drop.stats.RMSE),out.col] <- drop.stats.RMSE[nrow(drop.stats.RMSE),out.col] + validation.results$Root.Mean.Squre.Error
drop.stats.eRMSE[nrow(drop.stats.eRMSE),out.col] <- drop.stats.eRMSE[nrow(drop.stats.eRMSE),out.col] + validation.results$Equalised.RMSE
}#end if(geo)
} # end for each i.test
# Now average the results across cross-validation sets
drop.stats.D2[!is.na(drop.stats.D2[,out.col]),out.col] <- drop.stats.D2[!is.na(drop.stats.D2[,out.col]),out.col] / n.crossvalid.tests
drop.stats.RMSE[!is.na(drop.stats.RMSE[,out.col]),out.col] <- drop.stats.RMSE[!is.na(drop.stats.RMSE[,out.col]),out.col] / n.crossvalid.tests
drop.stats.eRMSE[!is.na(drop.stats.eRMSE[,out.col]),out.col] <- drop.stats.eRMSE[!is.na(drop.stats.eRMSE[,out.col]),out.col] / n.crossvalid.tests
# Work out which variable to drop, and drop it
if(n.preds>n.predictors.min)
{
if(test.col == 2)
{drop.var <- which.min(drop.stats.RMSE[,out.col])}
if(test.col == 3)
{drop.var <- which.min(drop.stats.eRMSE[,out.col])}
if(test.col == 4)
{drop.var <- which.max(drop.stats.D2[,out.col])}
if(geo){
if(drop.var<=n.preds){ # then we are dropping an env predictor
in.vars.in[drop.var] <- 0
n.preds<-n.preds-1
}else{ # then we must be dropping geo
geo<-FALSE
n.preds<-n.preds-1
}
}# end if (not sure why we had the 'else' below)
# }else{
#   in.vars.in[drop.var] <- 0
#   n.preds<-n.preds-1
# } # end else
}# end if n.preds>n.predictors.min
each.droplevel.D2[(n.crossvalid.tests+1),out.col] <- mean(each.droplevel.D2[c(1:n.crossvalid.tests),out.col])
out.col<-out.col+1
}# end for i.drp
###################################################################################
## Now we have a final model, run cross-validation with random test set         ##
if(n.preds <= n.predictors.min) # -- FINAL -- FINAL -- FINAL -- FINAL --
{
# Create a catcher for the final model
final.mod.MAE.set <- rep(0, times=n.crossvalid.tests)
final.mod.RMSE.set <- rep(0, times=n.crossvalid.tests)
final.mod.equRMSE.set <- rep(0, times=n.crossvalid.tests)
final.mod.D2.set <- rep(0, times=n.crossvalid.tests)
final.mod.MAE.rnd.set <- rep(0, times=n.crossvalid.tests)
final.mod.RMSE.rnd.set <- rep(0, times=n.crossvalid.tests)
final.mod.equRMSE.rnd.set <-rep(0, times=n.crossvalid.tests)
final.mod.D2.rnd.set <- rep(0, times=n.crossvalid.tests)
for(i.test in 1:n.crossvalid.tests)
{
# Grab the test and train site-pair data
Training.table.In <- train.lst[[i.test]]
Testing.table.In <- test.lst[[i.test]]
Testing.Rnd.table.In <- test.lst.rnd[[i.test]]
# Remove previously omitted variables
Training.table.In <- Training.table.In[,in.vars.cols]
Testing.table.In <- Testing.table.In[,in.vars.cols]
Testing.Rnd.table.In <- Testing.Rnd.table.In[,in.vars.cols]
# remove the rows with no data
Training.table.In<-Training.table.In[complete.cases(Training.table.In),]
Testing.table.In<-Testing.table.In[complete.cases(Testing.table.In),]
Testing.Rnd.table.In<-Testing.Rnd.table.In[complete.cases(Testing.Rnd.table.In),]
# For the applied sub-sampling scheme
validation.results.smp<- gdm_SingleCrossValidation(Training.table.In,
Testing.table.In,
geo=geo)
final.mod.MAE.set[i.test] <- validation.results.smp$Mean.Absolute.Error
final.mod.RMSE.set[i.test] <- validation.results.smp$Root.Mean.Squre.Error
final.mod.equRMSE.set[i.test] <- validation.results.smp$Equalised.RMSE
final.mod.D2.set[i.test] <- validation.results.smp$Test.Deviance.Explained
# For a purely random sample
validation.results.rnd<- gdm_SingleCrossValidation(Training.table.In,
Testing.Rnd.table.In,
geo=geo)
final.mod.MAE.rnd.set[i.test] <- validation.results.rnd$Mean.Absolute.Error
final.mod.RMSE.rnd.set[i.test] <- validation.results.rnd$Root.Mean.Squre.Error
final.mod.equRMSE.rnd.set[i.test] <- validation.results.rnd$Equalised.RMSE
final.mod.D2.rnd.set[i.test] <- validation.results.rnd$Test.Deviance.Explained
} # end for each i.test
} # end if n.preds <= n.predictors.min
###################################################################################
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
proc.time() - ptm
ptm <- proc.time()
## Now we have a final set of predictors, fit a full model sampling site-pairs from
## the full set of sites ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
# Set up catching matrices for the model parameters
in.vars <- in.vars[which(in.vars.in > 0)]
var.names <- var.names[which(in.vars.in > 0)]
n.params<-n.preds*3 # ASSUMING 3 KNOTS PER VARIABLE AT THE MOMENT
intercept.set<-rep(0, times=n.crossvalid.tests)
deviance.explained.set <- rep(0, times=n.crossvalid.tests)
final.mod.obs.dissim.evenness <- rep(0, times=n.crossvalid.tests)
coefficients.set<-matrix(0, nrow=n.crossvalid.tests, ncol=n.params)
knots.set<-matrix(0, nrow=n.crossvalid.tests, ncol=n.params)
final.mod.dissimilarity<-matrix(0, nrow=n.crossvalid.tests, ncol=6, dimnames=list(paste0("sample",c(1:n.crossvalid.tests)),c("Min.","1st Qu.","Median","Mean","3rd Qu.","Max.")))
# Loop through the samples of site pairs, fit GDMs, catch statistics
for(i.test in 1:n.crossvalid.tests)
{
## SUBSAMPLE SITE-PAIRS  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  ##
if(sample.method == 'random')
{
Pairs.Table.Train <- sitepair_sample_random(site.env.data = site.env.data,
n.pairs.target = n.pairs.train)
}#end if sample.method == 'random'
if(sample.method == 'geodist')
{
Pairs.Table.Train <- sitepair_sample_geographic(site.env.data = site.env.data,
n.pairs.target = n.pairs.train,
b.used.factor=b.used.factor,
b.dpair.factor=b.dpair.factor)
}#end if sample.method == 'geodist'
if(sample.method == 'envdist')
{
Pairs.Table.Train <- sitepair_sample_environment(site.env.data = site.env.data,
n.pairs.target = n.pairs.train,
b.used.factor=b.used.factor,
b.epair.factor=b.epair.factor)
}#end if sample.method == 'envdist'
if(sample.method == 'geodens')
{
Pairs.Table.Train <- sitepair_sample_density(site.env.data = site.env.data,
n.pairs.target = n.pairs.train,
domain.mask=domain.mask,
pcs.projargs=pcs.projargs,
b.used.factor=b.used.factor,
sigma.spair=sigma.spair,
b.spair.factor=spair.factor)
}#end if sample.method == 'geodens'
if(sample.method == 'geowt')
{
Pairs.Table.Train <- sitepair_sample_geo_weighted(site.env.data = site.env.data,
n.pairs.target = n.pairs.train,
bandwidth = bandwidth.geowt,
domain.mask = domain.mask,
pcs.projargs = pcs.projargs)
}# end if(sample.method == 'geowt')
##  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  ##
Pairs.Table.Train <- calculate_dissimilarities(pairs.table = Pairs.Table.Train,
composition.data = composition.data,
verbose=FALSE)
Pairs.Table.Train$s1.site.ID <- paste(Pairs.Table.Train$s1.decimalLongitude, Pairs.Table.Train$s1.decimalLatitude, sep = '_')
Pairs.Table.Train$s2.site.ID <- paste(Pairs.Table.Train$s2.decimalLongitude, Pairs.Table.Train$s2.decimalLatitude, sep = '_')
# Format these dataframes into GDM input tables
Training.table.In <- Pairs.Table.Train[,c(1:6)]
# Prepare predictor variables table
Training.GDM.input.vars <- matrix(0, nrow=nrow(Training.table.In), ncol=(length(in.vars)*2))
colnames(Training.GDM.input.vars) <- c(paste0("s1.",var.names),paste0("s2.",var.names))
# catch the env data for both sites in the pair
for(i.var in 1:length(in.vars))
{
Training.GDM.input.vars[,i.var] <- site.env.data[match(as.character(Pairs.Table.Train$s1.site.ID), as.character(site.env.data$xy)), (n.cols.start+in.vars[i.var])]
Training.GDM.input.vars[,(length(in.vars)+i.var)] <- site.env.data[match(as.character(Pairs.Table.Train$s2.site.ID), as.character(site.env.data$xy)), (n.cols.start+in.vars[i.var])]
} # end for i.var
# Join the variables to the site-pair table
Training.table.In <- cbind(Training.table.In, Training.GDM.input.vars)
Training.table.In<-Training.table.In[complete.cases(Training.table.In),]
# Fit a GDM [without warnings]
oldw <- getOption("warn")
options(warn = -1)
Final.mod <- gdm(Training.table.In,
geo=geo)
options(warn = oldw)
# Catch the parameters/stats
intercept.set[i.test] <- Final.mod$intercept
deviance.explained.set[i.test] <- Final.mod$explained
coefficients.set[i.test,] <- Final.mod$coefficients
knots.set[i.test,] <- Final.mod$knots
final.mod.dissimilarity[i.test,]<-summary(Training.table.In$distance)
dissim.hist <- hist(Training.table.In$distance, breaks=seq(from=0, to=1, by=0.025),plot=FALSE)
final.mod.obs.dissim.evenness[i.test] <- 1 - (DescTools::Gini(dissim.hist$counts))
} # end for i.test
#replace the final model object data with the mean values across models
Final.mod$intercept<-mean(intercept.set)
Final.mod$explained<-mean(deviance.explained.set)
Final.mod$coefficients<-colMeans(coefficients.set)
Final.mod$knots<-colMeans(knots.set)
##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##
proc.time() - ptm
r.time <- proc.time() - start.time
GDM_Builder_arguments=data.frame(argument=as.character(c('geo',
'train.proportion',
'n.pairs.train',
'n.pairs.test',
'n.crossvalid.tests',
'correlation.threshold',
'selection.metric',
'sample.method',
'Indiv.Dev.Explained.Min',
'n.predictors.min',
'b.used.factor',
'b.dpair.factor',
'b.epair.factor',
'sigma.spair',
'b.spair.factor',
'domain.mask',
'pcs.projargs',
'bandwidth.geowt',
'output.folder',
'output.name')),
value=c(as.character(rep(NA,times=20))))
GDM_Builder_arguments$value[1] <- as.character(geo)
GDM_Builder_arguments$value[2] <- as.character(train.proportion)
GDM_Builder_arguments$value[3] <- as.character(n.pairs.train)
GDM_Builder_arguments$value[4] <- as.character(n.pairs.test)
GDM_Builder_arguments$value[5] <- as.character(n.crossvalid.tests)
GDM_Builder_arguments$value[6] <- as.character(correlation.threshold)
GDM_Builder_arguments$value[7] <- as.character(selection.metric)
GDM_Builder_arguments$value[8] <- as.character(sample.method)
GDM_Builder_arguments$value[9] <- as.character(Indiv.Dev.Explained.Min)
GDM_Builder_arguments$value[10] <- as.character(n.predictors.min)
GDM_Builder_arguments$value[11] <- as.character(b.used.factor)
GDM_Builder_arguments$value[12] <- as.character(b.dpair.factor)
GDM_Builder_arguments$value[13] <- as.character(b.epair.factor)
GDM_Builder_arguments$value[14] <- as.character(sigma.spair)
if(!is.null(spair.factor)) {GDM_Builder_arguments$value[15] <- as.character(spair.factor)}
if(!is.null(domain.mask)) {GDM_Builder_arguments$value[16] <- as.character(domain.mask)}
domain.mask@title
domain.mask@file
domain.mask@file@name
GDM_Builder_arguments
GDM_Builder_arguments$value[1]
GDM_Builder_arguments$value[1] <- as.character(geo)
GDM_Builder_arguments=data.frame(argument=as.character(c('geo',
'train.proportion',
'n.pairs.train',
'n.pairs.test',
'n.crossvalid.tests',
'correlation.threshold',
'selection.metric',
'sample.method',
'Indiv.Dev.Explained.Min',
'n.predictors.min',
'b.used.factor',
'b.dpair.factor',
'b.epair.factor',
'sigma.spair',
'b.spair.factor',
'domain.mask',
'pcs.projargs',
'bandwidth.geowt',
'output.folder',
'output.name')),
value=as.character(rep(NA,times=20)))
GDM_Builder_arguments
GDM_Builder_arguments$value[1]
GDM_Builder_arguments$value <- as.character(GDM_Builder_arguments$value)
GDM_Builder_arguments
GDM_Builder_arguments$value[1]
GDM_Builder_arguments$value[1] <- as.character(geo)
GDM_Builder_arguments$value[2] <- as.character(train.proportion)
GDM_Builder_arguments$value[3] <- as.character(n.pairs.train)
GDM_Builder_arguments$value[4] <- as.character(n.pairs.test)
GDM_Builder_arguments$value[5] <- as.character(n.crossvalid.tests)
GDM_Builder_arguments$value[6] <- as.character(correlation.threshold)
GDM_Builder_arguments$value[7] <- as.character(selection.metric)
GDM_Builder_arguments$value[8] <- as.character(sample.method)
GDM_Builder_arguments$value[9] <- as.character(Indiv.Dev.Explained.Min)
GDM_Builder_arguments$value[10] <- as.character(n.predictors.min)
GDM_Builder_arguments$value[11] <- as.character(b.used.factor)
GDM_Builder_arguments$value[12] <- as.character(b.dpair.factor)
GDM_Builder_arguments$value[13] <- as.character(b.epair.factor)
GDM_Builder_arguments$value[14] <- as.character(sigma.spair)
if(!is.null(spair.factor)) {GDM_Builder_arguments$value[15] <- as.character(spair.factor)}
if(!is.null(domain.mask)) {GDM_Builder_arguments$value[16] <- as.character(domain.mask@file@name)}
if(!is.null(pcs.projargs)) {GDM_Builder_arguments$value[17] <- as.character(pcs.projargs)}
if(!is.null(bandwidth.geowt)) {GDM_Builder_arguments$value[18] <- as.character(bandwidth.geowt)}
if(!is.null(output.folder))GDM_Builder_arguments$value[19] <- as.character(output.folder)
GDM_Builder_arguments$value[20] <- as.character(output.name)
GDM_Builder_results = list(Inputs = match.call(),
Arguments = GDM_Builder_arguments,
ProcessingTime = r.time,
Individual.Performance = ind.var.test.stats.sampairs,
Backward.Elim.D2 = drop.stats.D2,
Backward.Elim.RMSE = drop.stats.RMSE,
Backward.Elim.eRMSE = drop.stats.eRMSE,
Backward.Elim.Full.D2 = each.droplevel.D2,
#Predictors = Final.mod$predictors,
Deviance.Explained = deviance.explained.set,
Intercept = intercept.set,
Dissimilarities.Evenness = final.mod.obs.dissim.evenness,
Mean.Absolute.Error = final.mod.MAE.set,
Root.Mean.Squre.Error = final.mod.RMSE.set,
Equalised.RMSE = final.mod.equRMSE.set,
Deviance.Exp.CrossVal = final.mod.D2.set,
rnd.Mean.Absolute.Error = final.mod.MAE.rnd.set,
rnd.Root.Mean.Squre.Error = final.mod.RMSE.rnd.set,
rnd.Equalised.RMSE = final.mod.equRMSE.rnd.set,
rnd.Deviance.Exp.CrossVal = final.mod.D2.rnd.set,
dissimilarity.summary = final.mod.dissimilarity,
Mean.Final.GDM = Final.mod)
GDM_Builder_arguments
## Write the output list to file if specified ##
if(!is.null(output.folder))
{
if(!dir.exists(output.folder))
{
dir.create(output.folder)
}# end if !dir.exists
out.path <- file.path(output.folder,paste0(output.name,"_",Sys.Date(),".Rdata"))
save(GDM_Builder_results, file=out.path)
}#end if !is.null(output.folder)
library(devtools)
library(roxygen2)
library(Rcpp)
pkg_root = '//ces-10-cdc/OSM_CDC_MMRG_work/users/bitbucket/gdm_workflow/gdmEngine'
## write DESCRIPTION file
DESCRIPTION = c('Package: gdmEngine',
'Version: 0.01',
paste('Date:', Sys.Date()),
'Title: Workflow for GDM',
'Description: Functions used to develop GDMs',
paste('Author:', unname(Sys.info()['user'])),
'Maintainer: Chris Ware <chris.ware@csiro.au>',
'SystemRequirements: git with shell distribution',
'Licence: errr',
#paste('Authors@R:', unname(Sys.info()['user']))
'Imports: Rcpp (>= 0.11.4)',
'LinkingTo: Rcpp'
)
sink(paste(pkg_root, 'DESCRIPTION', sep = '/'))
cat(DESCRIPTION, sep = '\n')
sink()
## Build with devtools
setwd(pkg_root)
document()
build()
install(quick = TRUE)
gitr.push(files = 'all')
